[
{
	"uri": "/releases/version2/",
	"title": "Migrate from v1 to v2",
	"tags": [],
	"description": "",
	"content": "At playground.digital.auto, we\u0026rsquo;ve listened to your feedback and identified some limitations in the architecture of v1. To address these, we\u0026rsquo;re excited to announce a new version with significant improvements!\nPython applications utilize the standard Velocitas library. Python applications can leverage external libraries, enabling the development of a wide range of applications. Widgets are fully plug-and-play, with no dependency on any specific model. UX improvements enhance user-friendliness. Rust Language Support: Write, compile, and execute Rust code with a single click. Supports 100 popular Rust libraries by default. New Feature: Staging replaces the previous Deploy dialog, simplifying the management of deployments from the development environment to target hardware. SDV Runtime: A Docker image serves as the execution environment for SDV applications, accommodating both Python and Rust applications. Users can either utilize numerous shared runtimes provided by us or deploy their own runtime on their local machine or in the cloud. Asset Management: Users can add and manage their own SDV runtimes or hardware kits, ensuring exclusive usage and controlled sharing. Furthermore, users can share their assets with collaborators. To accommodate the aforementioned features, the underlying architecture of the entire playground has undergone a complete overhaul. Please take note of the following points when transitioning to version 2.\n1. The Python code is formatted differently now Below is sample code for simple head light blinking app.\nVersion 1 from sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() while True: await vehicle.Body.Lights.Beam.Low.IsOn.set(True) await aio.sleep(1) await vehicle.Body.Lights.Beam.Low.IsOn.set(False) await aio.sleep(1) Version 2 from vehicle import Vehicle import time import asyncio import signal from sdv.vdb.reply import DataPointReply from sdv.vehicle_app import VehicleApp from vehicle import Vehicle, vehicle class TestApp(VehicleApp): def __init__(self, vehicle_client: Vehicle): super().__init__() self.Vehicle = vehicle_client async def on_start(self): while True: await asyncio.sleep(1) await self.Vehicle.Body.Lights.Beam.Low.IsOn.set(True) await asyncio.sleep(1) await self.Vehicle.Body.Lights.Beam.Low.IsOn.set(False) await asyncio.sleep(1) async def main(): vehicle_app = TestApp(vehicle) await vehicle_app.run() LOOP = asyncio.get_event_loop() LOOP.add_signal_handler(signal.SIGTERM, LOOP.stop) LOOP.run_until_complete(main()) LOOP.close() As you may have noticed, version 1 simplified the code style to facilitate a quick understanding of the Vehicle API concept. However, this simplified approach precluded execution on actual hardware. In version 2, we strictly adhere to the Velocitas library and templates, enabling code execution on real hardware. This bridges the gap between the playground and hardware setup. The code you write and run in the playground will now accurately reflect the behavior on the target hardware.\nOn version 2:\nThe app is now a class that inherit standard class VehicleApp. Your code will be trigger from function: async def on_start(self): Vehicle API set/get/subcrible is followed below formated: # write an actuator signal with value await self.Vehicle.Body.Lights.Beam.Low.IsOn.set(True) # read an actuator back value = (await self.Vehicle.Body.Lights.Beam.Low.IsOn.get()).value print(\u0026#34;Light value \u0026#34;, value) ... # subscibe for signal value change async def on_light_value_changed(self, data: DataPointReply): print(\u0026#34;Detect that light value changed\u0026#34;) new_value = data.get(self.Vehicle.Body.Lights.Beam.Low.IsOn).value print(\u0026#34;Value change to\u0026#34;, new_value) async def on_start(self): await self.Vehicle.Body.Lights.Beam.Low.IsOn.subscribe(self.on_light_value_changed) ... 2. The plugin mechanism has been deprecated To enhance clarity and adhere to the plug-and-play principle, widgets are now decoupled from any specific model. Python code and widgets operate with complete isolation.\n[widgets] \u0026lt;=\u0026gt; Vehicle API \u0026lt;=\u0026gt; [SDV App]\nData exchange between widgets and your application must exclusively occur through the Vehicle API.\n3. Your Python code runs within SDV Runtime, which is a Docker container Upon clicking the Run button, your code is executed within a runtime environment. The runtime processes your code and streams the output back to the dashboard. Consequently, it is crucial to carefully select the appropriate runtime environment before running your application.\nWe provide a selection of shared runtimes for user convenience. These pre-configured environments allow users to readily execute their applications. Alternatively, users have the flexibility to configure and deploy their own custom runtime environments.\nHow to migrate your prototypes from v1 to v2 By default, when you log in to v2 for the first time, your data from v1 will not be transferred. This is because the data structure and underlying infrastructure are completely different, and we cannot guarantee a seamless automatic migration.\nWe encourage you to explore the improved features of version 2 and create new prototypes for your use cases. However, if you need to transfer your prototypes from v1 to v2, you can follow the guidance in this video.\nGive your feedback To provide feedback or request support click here\n"
},
{
	"uri": "/advanced/ai_sdv_app/ai_app_on_pg/",
	"title": "AI App Concept",
	"tags": [],
	"description": "",
	"content": "Preface This document for AI engineer who familiar with AI application development concept.\nIt assumes you have basic understanding on Vehicle API concept, we provide simple explanation at this VSS Basic Documentation.\nThe purpose of this document is to discuss AI-on-Edge, means realtime AI running directly on vehicle, not apply for AI-on-Cloud like ChatGPT\u0026hellip;\nAI application on playground Nowadays, AI is a hot topic. There are plenty of tools, libraries and methods to build and deploy an AI application or reuse AI service from 3rd provider. In the scope of this tutorial, we discuss about the process to build an AI application by your own, test it on digital.auto playground, and deploy it to PoC HW such as dreamKIT\u0026hellip;\nThere are many ways to deploy an AI app for vehicle, the diagram below is a suggestion on how to use AI with Vehicle APIs.\nIdeally, the vehicle application developed on digital.auto playground could be executed on edge without any modification. This is enabled by the abstracted vehicle APIs, and container technology.\nThe power of API abstraction gives us the freedom to implement AI a little bit different (or absolutely different) on each environment. On web, digital.auto playground, we are limited by Javascript runtime, so we should go with TensorFlow.JS. TensorFlowJS, by using WASM, can access your GPU to accelerate the calculation. For an AI vision app, most of the time input will be an image and your trained model. Then you can set the output to Vehicle API. From now on, all the vehicle app can get Vehicle value and implement their logic.\nNext, moving your Vehicle App to PoC HW to testing. On this context, you need an AI service to turn image stream to API value. You can use TensorFlow again (to reuse the sample AI model) or other tools such as PyTorch. It depends on your HW environment, license, cost and plenty of other factors.\n"
},
{
	"uri": "/advanced/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "Getting started Please have a look at image below.\nThis architecture has 2 parts:\n(1) Playground general architecture (2) Architecture and flow from Playground to dreamKIT This page is focused on (1) Playground general architecture. For more information about (2) Architecture and flow from Playground to dreamKIT, please refer Playground to dreamKIT.\nPlayground general architecture The playground is a cloud-based web application that is responsible for rapidly prototyping environment for new, SDV-enabled features.\nTo bring SDV-vehicle development experience to website, we are currently using these technologies and tools on playground:\nFront-end:\nReact: Front-end library TailwindCSS: CSS framework Back-end:\nNetlify: Utilize for server-side functions such as authentication and permissions Firebase Firestore: Database Other:\nBrython: Allow to run Python code in browser environment Socket.IO: Real-time bidirectional communication Web Assembly: Execute high performance, low level code in browser COVESA VSS: Syntax and catalog for vehicle signal Velocitas: Toolchain to create containerized in-vehicle application Dive deeper into playground Please have a glance at below picture. This picture describes components and how things work in the Playground dashboard.\nBefore coming to what a Playground dashboard is, let\u0026rsquo;s take a look at some of the components in the image above:\nVSS-API: APIs that adhere to the format of COVESA Vihicle Signal Specification. You can also create your custom APIs. Simulator: Provide simulation for VSS-APIs. This is written in Python and later translated to Javascript code to execute within browser environment. More information please refer How Python-Javascript works code.py: Python script responsible for interacting with the VSS-API and handle associated logics. Widget: UI apps that fetch data from VSS-API and display them. There are 2 types of Widgets: The built-in widgets Custom widgets: These are managed and published at marketplace AI Engine: 3rd-party services such as LandingAI So what is Playground dashboard?\nDashboard Diagram Dashboard Config on Playground Actual Dashboard on Playground Playground dashboard is where you can place your Widgets. Dashboard has 10 tiles. A widget can be placed on one or many tiles. You also have options to config the widget, such as which APIs this widget are interacting with.\nThe server migration In aforementioned architecture, 3rd-party services (Firebase and Netlify) enhance the speed of development. However, as the application scales, this part of system transitions from a facilitator to a burden, making optimization more challenging and introducing additional complexities to the development process. That is why we are in progress to migrate these serverless platform to our self-managed server (using NodeJS and MongoDB)\n"
},
{
	"uri": "/campaign/gen-ai-awards-2024/1-getting-started/",
	"title": "Getting started",
	"tags": [],
	"description": "",
	"content": "Great to have you join the digital.auto GenAI Awards!\nWe are on the lookout for innovative ways to apply multimodel GenAI in our web-based prototyping environment. The goal is to create visually rich SDV prototypes and solution sketches.\nSubmit your GenAI-driven SDV idea and win up to €5,000! With quarterly competitions, each round offers a chance to win €2,000, and the final showdown between the 3 best teams adds another €3,000 to the pot.\nPlease refer to https://www.digital.auto/genai-award to learn more about the genAI Awards Competition. You can navigate to the idea submission from that landing page as well.\n"
},
{
	"uri": "/advanced/how-python-javascript-works/",
	"title": "How Python-Javascript works",
	"tags": [],
	"description": "",
	"content": "Prerequisite To understand how this Python-Javascript works, you need some foundation knowledge of HTML, Javascript, and Python. You also need to read Create \u0026lsquo;Hello World\u0026rsquo; Prototype guide or at least have some experience with prototype on Playground before reading this documentation.\nSimilarity of Python code on Playground No matter what prototype and model is being used, every Python code on Playground share the same structure. They always contain this line of code:\nfrom sdv_model import Vehicle Below is a snapshot of Python code on Playground. In this Python code, you can clearly see a Vehicle class is imported from sdv_model module.\nThis Vehicle class serves as a core component. It is responsible for simulating all APIs within prototype and facilitating every APIs call in the code. Understanding the implementation of the Vehicle class is a must for gaining insight into how Python-Javascript works.\nVehicle class implementation\nThis class is written in Python.\nVehicle class is capable of:\nRecognizing and interacting with APIs in the prototype (APIs can be VSS API or Custom/Wishlist APIs). Simulating of all states and values associated with the APIs. Facilitating of API calls, including get, set, and subscribe operations. Validating of data types passed to API calls. Later the Vehicle class is converted to Javascript code using Brython library. Then it is saved as a file at https://digitalauto.netlify.app/brython/sdv_model.brython.js\nThe below picture depicts the interaction between Python code (code.py) and the Vehicle class (simulator) How Python code on playground is executed Within a prototype, proceed to the Dashboard tab and click the Run button. This action triggers the execution of the Python code written in the Code tab. The Widgets should then be able to detect the changes made by your Python code to the APIs. Again, please refer Create \u0026lsquo;Hello World\u0026rsquo; Prototype guide to understand what we are doing. The Python code itself cannot directly run on browser. Thus, a series of steps must be undertaken to enable its execution. For more detail, the following approach is adopted:\nFirst, an iframe was created. This iframe contains all of these things:\nThe Python code in the Code tab\nThe Brython scripts are embedded through CDN. It allows us to run Python code in browser:\n\u0026lt;script referrerpolicy=\u0026#34;origin\u0026#34; src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/brython/3.10.5/brython.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script referrerpolicy=\u0026#34;origin\u0026#34; src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/brython/3.10.5/brython.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Finally, the aforementioned Vehicle class:\n\u0026lt;script referrerpolicy=\u0026#34;origin\u0026#34; src=\u0026#34;https://digitalauto.netlify.app/brython/sdv_model.brython.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; After that, utilize Brython library, the Python code in the Code tab will be converted to Javascript.\nFinally, execute Javascript code using eval function.\nWhy we need to create a separate iframe?\nDue to the utilization of the eval function for execution, the code must be within a separate iframe to prevent the potential risks of dangerous or destructive actions. The Brython library and the Javascript itself don\u0026rsquo;t have a native mechanism to stop the running code (in this case it\u0026rsquo;s a Javascript Promise), for example, a infinite loop code block. Put it in a separate iframe will make it a lot easier to stop it when required. "
},
{
	"uri": "/engaged/helloworld/",
	"title": "Create &#39;Hello World&#39; Prototype",
	"tags": [],
	"description": "",
	"content": "A software engineer typically starts programming journey by printing \u0026ldquo;hello world\u0026rdquo; on the terminal. An embedded software engineer typically begins first MCU programming by blinking a LED. That\u0026rsquo;s a good moment to remember. In this section, to get started with digital.auto playground, we do the same basis but memorable thing - blinking your vehicle headlight.\nBefore going to detail, here is a summary of what we shall do in this section:\nCreate an account if you don\u0026rsquo;t have one. Or you can login by Github. Create a Vehicle Model (aka Model). Create a Prototype. On above prototype, write a simple code to blink the headlight. Configure the dashboard to show the status of headlight. Go to dashboard, enjoy the result. It takes 5 minutes to complete the entire process.\n1. Login Visit digital.auto playground at https://playground.digital.auto If you don\u0026rsquo;t have an account, you can login by github or register new account, reference registration guide for more detail\n2. Create a Vechicle Model In digital.auto, model is the umbrebla for everything you do. One model contains a list of vehicle APIs, and a collection of prototypes. In short, model has its own API list, and some protypes shall use that API list. As a free user, you can create up to 3 models. Create a new one if you haven\u0026rsquo;t had yet. If you have a model, click on it to select and go to step 3. If not, click Create New Model. On Create Prototype Panel, enter a name. For API, leave it as is. And for visibility, select Pulic option if you want to share your model to community and everyone can see it. Select private option if you only want you or your team in contributor list to get access and other users shall not see it.\n3. Create a new Prototype First, click on Prototype list, we are going to create a new prototype now.\nClick + New Prototype at bottom left corner, then give it a name.\n4. Write simple headlight-blinking code In prototype view, click on Tab Code. You shall find some initial code already created by default.\nLet add some simple code as follows:\nfor i in range(10): # code to turn headlight on await aio.sleep(1) # code to turn headlight on await aio.sleep(1) You can guess, we just loop for ten times to turn headlight on/off. And await aio.sleep(1) is a simple method to sleep for 1 second. Now you need to add lines of code to turn headlight on and off.\nOn the right panel, with APIs tab, search for Vehicle.Body.Lights.IsLowBeamOn\nOn new popup, we have sample code for Get API value, Set API value, and Subscribe for value changed. Click copy on Set function.\nBack to python code, paste it to the placehoder and change value as follows:\nfor i in range(10): # code to turn headlight on await vehicle.Body.Lights.IsLowBeamOn.set(True) await aio.sleep(1) # code to turn headlight on await vehicle.Body.Lights.IsLowBeamOn.set(False) await aio.sleep(1) That\u0026rsquo;s it for the code.\n5. Configure the dashboard Now it\u0026rsquo;s time to configure the dashboard to show the headlight status when we run the python code.\n6. Run the code Go to dashboard tab, click run button and watch the result\nVoila! You are done with your first prototype. Don\u0026rsquo;t worry if you don\u0026rsquo;t understand in detail. On next tutorials we shall go in depth on each point.\n"
},
{
	"uri": "/engaged/3dcar/",
	"title": "Using &#39;General 3D Car Model&#39; Widget",
	"tags": [],
	"description": "",
	"content": "Introduction General 3D Car Model widget is designed to enhance your experience in developing software for software-defined vehicles (SDV). It allows you to interact with a realistic 3D model of a sedan, complete with customizable features via Vehicle APIs. This guide will walk you through the steps to integrate and use this widget in your SDV applications.\nHere\u0026rsquo;s a quick overview of what we\u0026rsquo;ll cover:\n1. Adding the widget to your dashboard from the Marketplace\n2. Configuring the widget\n3. Running code and interacting with the 3D environment\n4. Exploring the features in-depth\n1. Add widget to dashboard from Marketplace Start by selecting the widget from our marketplace. Make sure to allocate at least 6 cells on your dashboard for an optimal visual experience.\nDashboard Configuration:\nChoose a minimum of 6 cells for placement.\nAdding the Widget:\nSelect \u0026lsquo;General 3D Car Model\u0026rsquo; from the marketplace.\n2. Configuring the widget The widget comes with default options. For example, VEHICLE_PAINT lets you change the car\u0026rsquo;s color using hex values. The PROXIMITY_API should be linked with your Vehicle API to reflect the driver-vehicle distance. The VIEWPOINT offers 9 perspectives, enhancing the 3D experience. The door, seat, and trunk features are tied to specific Vehicle APIs, which we\u0026rsquo;ll detail in Section 4.\nNote: If there is no support for the proximity value you need to set up a wishlist API for the proximity value, using a float data type (e.g., Vehicle Proximity).\n{ \u0026#34;VEHICLE_PAINT\u0026#34;: \u0026#34;#DEE2E6\u0026#34;, \u0026#34;PROXIMITY_API\u0026#34;: \u0026#34;Vehicle.Proximity\u0026#34;, \u0026#34;VIEWPOINT\u0026#34;: 4 } The following is a Python SDV code snippet for a \u0026lsquo;Smart Door\u0026rsquo; prototype, demonstrating the widget\u0026rsquo;s capabilities:Prototype Smart Door\nfrom sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() # Init default value await vehicle.Cabin.Seat.Row1.Pos1.Position.set(0) await vehicle.Cabin.Door.Row1.Left.IsOpen.set(False) proximity = await vehicle.Proximity.get() await aio.sleep(1) while(1): proximity = await vehicle.Proximity.get() print(\u0026#34;Proximity\u0026#34;, proximity) if proximity \u0026lt; 10: await vehicle.Cabin.Door.Row1.Left.IsOpen.set(True) print(\u0026#34;Open Door\u0026#34;) await aio.sleep(1.5) print(\u0026#34;Expand Seat\u0026#34;) await vehicle.Cabin.Seat.Row1.Pos1.Position.set(10) await aio.sleep(1.5) await aio.sleep(1) 3. Run code and interact with 3D environment Using the provided SDV Python code, you can experience the widget\u0026rsquo;s features firsthand.\nViewpoint Options: Choose from 9 viewpoints to customize your view from the driver\u0026rsquo;s perspective.\nControls: Use forward and backward buttons to simulate proximity changes.\nDashboard\nInitial view with VIEWPOINT: 4, showing the driver\u0026rsquo;s door and the proximity level is shown in Line chart and Single API widgets\nRunning the code\nWhen you press Run button. As you press forward and backward buttons simulate approaching the vehicle, watch the proximity level change and trigger door opening and seat adjustments based on your SDV Python code.\n4. Deep Dive into Features Let\u0026rsquo;s explore each feature in more detail: 4.1 Door Control Description: Control the opening and closing of the first-row doors (left and right) Operation: Use boolean values (true/false) to toggle the state of the doors Associate API: Left door: Vehicle.Cabin.Door.Row1.Left.IsOpen Right door: Vehicle.Cabin.Door.Row1.Right.IsOpen 4.2 Trunk Control Description: Control the opening and closing of the trunk Operation: Use boolean values (true/false) to toggle the state of the trunk Associate API: Vehicle.Body.Trunk.Rear.IsOpen 4.3 Seat Adjustment Description: Adjust the position of the seats in the first row. Operation: Use int values from 0 to 10 to adjust the seat position (0: Close to driving wheel - 10: Far from driving wheel) Associate API: Left seat: Vehicle.Cabin.Seat.Row1.Pos1.Position Right seat: Vehicle.Cabin.Seat.Row1.Pos2.Position 4.4 Customization Options Change the car model color using hex values (e.g.: Indigo Blue: #005072, Platinum White: #DEE2E6, Obsidian Black: #212529, \u0026hellip;) This guide aims to provide you with a clear and straightforward understanding of how to use the \u0026lsquo;General 3D Car Model\u0026rsquo; widget. Happy coding!\n"
},
{
	"uri": "/engaged/simplelandingai/",
	"title": "Using &#39;Simple LandingAI&#39; Widget",
	"tags": [],
	"description": "",
	"content": "Introduction This Landing AI widget allows users to import Landing AI models and use their prediction result as API value to control vehicle functions, streamlining the presentation of ideas about their SDV application without the need for in-depth web code manipulation.\nHere we take demo case as an example to demonstrate how to create an AI-related Software-defined Vehicle prototype. The process is divided into two main parts, building an object detection AI model on the Landing AI platform LandingAI from scratch and deploying this model on digital.auto Playground.\nOverview The process is outlined in the following steps: Step 1. Prepare LandingAI model 1.1 Register a LandingAI account and create an AI project Register an account at Landing AI homepage (https://app.landing.ai/login). After entering your personal account home page, click on Create Project title to create your project. After entering your created project, click on Edit icon and enter a descriptive name for your project. 1.2 Select your project type Three main Computer Vision related tasks that can be developed on LandingAI: Object Detection: I want to identify objects by drawing boxes around them. Segmentation: I want to identify objects by painting pixels on them. Classification: I want to identify each image into different categories. For Wallet Detection project, we select the project type of Object detection. 1.3 Upload your image dataset If you have labeled image datasets, you can upload them by selecting drop to upload button. Drag the image dataset to LandingAI on the upload image page. After uploading your dataset to LandingAI, the images will be automatically annotated. If you only have some raw images without labeling, you can utilize LandingAI online annotation tool to label your images. Update your raw images to LandingAI as before, then click one image to enter the annotation editor. Click on + Create Class to define the item you want to detect into a class, e.g., Wallet.\nYou can label your images by drawing a box on the object that you want to identify, then press the left or right arrow keys on your keyboard accordingly to move to the next image. If the image does not have any items to label, then click Nothing to Label on the bottom right corner. 1.4 Train your model online After you prepare your dataset on LandingAI, click the Train button to start training your customized wallet detection model, which will take a few minutes. 1.5 Generate your model API key Click on Deploy button in the left menu, then click on Create new endpoint and deploy. Give a name and click Create button to create your endpoint. Select View API Key, enter the name, and click Generate API Key to generate your personalized API key. Later during the deployment of digital.auto Playground, you will use API calls to send images by specifying your API key. Step 2: Configure widget Configure the widget using the JSON structure below to set up AI interaction and API control parameters.\n{ \u0026#34;options\u0026#34;: { \u0026#34;endpoint_id\u0026#34;: \u0026#34;\u0026lt;your_endpoint_id\u0026gt;\u0026#34;, \u0026#34;landingAI_apikey\u0026#34;: \u0026#34;\u0026lt;your_api_key\u0026gt;\u0026#34;, \u0026#34;set_to_api\u0026#34;: \u0026#34;\u0026lt;your_API_endpoint\u0026gt;\u0026#34;, \u0026#34;datatype\u0026#34;: \u0026#34;\u0026lt; boolean | float | integer \u0026gt;\u0026#34;, \u0026#34;threshold\u0026#34;: \u0026#34;\u0026lt;threshold_value\u0026gt;\u0026#34; }, } endpoint_id: Your unique identifier for the AI model, allowing the import of trained models into the widget. landingAI_apikey: A private API key for authentication, ensuring secure access to your AI model. set_to_api: The specific API endpoint that you want to interact with using the widget. This could be an API for controlling vehicle functionalities, like turning lights on or off. datatype: If there is no threshold to toggle true or false. You can select between integer and float. threshold: A predefined value that determines when the set_to_api action should be triggered. If the AI model\u0026rsquo;s prediction score exceeds this threshold, the set_to_api field will be set to true. When there is no threshold, the API is set value based on the score and datatype. Step 3: Upload/capture Image Upload or capture images for immediate processing and prediction. Capture Image Upload Image Step 4: Predict Inference LandingAI walet detection model with both capture and upload images Capture Image Upload Image Step 5: Return result as API value Based on the inference result on capture image and defined threshold, the widget sets the Vehicle.Interior.ItemLeftBehindDetection.IsActive value to true when the detection result from the Landing AI return score 92%\n"
},
{
	"uri": "/advanced/ai_sdv_app/ai_getting_started/",
	"title": "AI with playground",
	"tags": [],
	"description": "",
	"content": "Introduction AI becomes more and more popular in daily life, and definitely it is also a trend in the automotive industry. In this section, we will introduce how to use AI in the playground.\nNote: AI have a lot of different applications, and we will only introduce a simple image processing use case in this section.\nWe assume that you already know how to create account, model and prototype in the playground. If not, please refer this Helloworld section.\n1. Go to the playground, and login with your account. 2. Create a new model, if you don\u0026rsquo;t have one. 3. Create a new prototype Make sure you select the model you just created.\n4. Config dashboard Go to the prototype page, and click the \u0026ldquo;Code Tab\u0026rdquo; and on the right side, click \u0026ldquo;Dashboard Config\u0026rdquo; tab. Then pick a widget, and select \u0026ldquo;Driver Distraction\u0026rdquo; widget, place it on the dashboard. Then you have a \u0026ldquo;Dashboard Config\u0026rdquo; as below:\n[ { \u0026#34;plugin\u0026#34;: \u0026#34;Builtin\u0026#34;, \u0026#34;widget\u0026#34;: \u0026#34;Driver-Distraction\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;draw_boundary\u0026#34;: true, \u0026#34;set_to_api\u0026#34;: \u0026#34;Vehicle.Driver.DistractionLevel\u0026#34; }, \u0026#34;boxes\u0026#34;: [ 1, 2, 7, 6 ] } ] 5. Use AI Widget Switch to \u0026ldquo;Dashboard\u0026rdquo; tab, give some seconds for the widget and AI model to load, and you will see a \u0026ldquo;Driver Distraction\u0026rdquo; widget on the dashboard.\nIf the browser ask you to allow the camera, please allow it. The widget need to access your camera to capture the image.\nAfter AI model successfully loaded, you will see the result, try to use your phone, put it in your ear, AI widget can detect you are using phone or not. Base on that, it will give you a distraction level. The distraction level is a number between 0 and 100, 0 means you are not distracted at all, 100 means you are distracted. This level will auto set to api: \u0026ldquo;Vehicle.Driver.DistractionLevel\u0026rdquo;.\nYou can call this api in your app, and do some actions base on the distraction level.\nYou can tell the widget to set the distraction level to any api you want, just change the \u0026ldquo;set_to_api\u0026rdquo; option in the \u0026ldquo;Dashboard Config\u0026rdquo; tab.\nNo phone used result\nPhone used result\n6. Verify API result Display the distraction level result in another widget.\nGo back to \u0026ldquo;Dashboard Config\u0026rdquo;, pick a new widget, and select \u0026ldquo;Single-API-Value\u0026rdquo; widget, place it on the dashboard. In new widget option, change to name and API to \u0026ldquo;Distraction Level\u0026rdquo;, and API to \u0026ldquo;Vehicle.Driver.DistractionLevel\u0026rdquo; as below.\n{ \u0026#34;plugin\u0026#34;: \u0026#34;Builtin\u0026#34;, \u0026#34;widget\u0026#34;: \u0026#34;Single-API-Widget\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;label\u0026#34;: \u0026#34;Distraction Level\u0026#34;, \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Driver.DistractionLevel\u0026#34;, \u0026#34;labelStyle\u0026#34;: \u0026#34;color:black;font-size:20px\u0026#34;, \u0026#34;valueStyle\u0026#34;: \u0026#34;color:teal;font-size:30px;\u0026#34;, \u0026#34;boxStyle\u0026#34;: \u0026#34;background-color:white;\u0026#34; }, \u0026#34;boxes\u0026#34;: [ 3 ] } Go back to \u0026ldquo;Dashboard\u0026rdquo; tab, you will see the distraction level result in the new widget.\n7. Write application code Write some code to do some actions base on the distraction level. In the \u0026ldquo;Code\u0026rdquo; tab, you can write your python code in the left panel. Try to using distraction level to control HVAC Fan.\nfrom sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() stop = 0 full = 100 while True: level = await vehicle.Driver.DistractionLevel.get() if level\u0026gt;50: await vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed.set(full) await aio.sleep(3) else: await vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed.set(stop) await aio.sleep(1) 8. Add a Fan widget to the dashboard Go back to \u0026ldquo;Dashboard Config\u0026rdquo;, pick a new widget, and select \u0026ldquo;Fan-Widget\u0026rdquo;, place it on the dashboard. In widget option, mention the API you using in python code.\n{ \u0026#34;plugin\u0026#34;: \u0026#34;Builtin\u0026#34;, \u0026#34;widget\u0026#34;: \u0026#34;Fan-Widget\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed\u0026#34; }, \u0026#34;boxes\u0026#34;: [ 8 ] } 9. Run the application Go to \u0026ldquo;Dashboard\u0026rdquo; tab, click \u0026ldquo;Run\u0026rdquo; button, and you will see the fan speed change base on the distraction level.\nYou can follow this video for step by step guide:\nHow the AI widget works The AI widget is a wrapper of the Tensorflow.js library.\nStep 1: We train a model with Tensorflow, and export it to a format that tensorflowjs can be used.###\nStep 2: We use the tensorflowjs to load the model, and run it in the browser. Capture the image continously, and run the model to get the result(distraction_level)\nStep 3: The widget will set distraction_level to the api, via a special mechanism provide by digital.auto so that a widget can set a value to an API. This is a builti in mechanism, and you don\u0026rsquo;t need to care about it. Please refer Create Custom Widget tutorial for more detail.\nAnd from now on, you can use the distraction_level via API in your application code, and do some actions base on it.\nSource code for Driver Distraction Widget You can find the source code for the Driver Distraction Widget here\nThis is just an example, and you can use the same method to apply AI in other use cases.\nThe are plenty of models available in the internet, and you can use them in the widget.\nYou can also train your own model, and use it in the widget. Please refer to Tensorflow.js for more detail.\nYou can also using another method to have an AI engine running and give the result to widget. This tutorial is just one of the way to do it.\nHow about real vehicle envinronment? On real vehicle, we need to consider the performance of the AI model, and the performance of the hardware.\nOn vehcile context, python code(or C++) keep the sample logic. And the AI model is running on a separate hardware or seperate app/service, and the result is send to the python code via API. The python code will do some actions base on the result from the API.\n"
},
{
	"uri": "/campaign/gen-ai-awards-2024/2-onboarding/",
	"title": "Onboarding",
	"tags": [],
	"description": "",
	"content": "Before you can build, test and submit your genAI, you need to register your team first.\nGo to https://genai.digital.auto/\nYou need to login first to do further steps. You can login with a GitHub account. Fill your team\u0026rsquo;s information. Every team member should already have an account. Your team can have up to 5 members. NOTE: The campain is limited to NONemployee of companies in the list of sponsorship.\nApply for Amazon Web Service Bedrock credits.\n4.1. Apply for AWS Bedrock credits. This is to get free credits from AWS - as one major sponsor of the campain - for your genAI development and testing. Providing a clear description of your general idea in step 3 is crucial for evaluation. Failing to adequately describe your idea may result in being denied access to AWS Bedrock. Optionally, you can choose to use your own generative AI service if you don\u0026rsquo;t like to get credit for free. 4.2. Waiting for approval. The review process can take up to 24 hours. If approved, we will send a email to inform you. Review your team. Once your application is approved, you can find your credentials here. We provide you IAM user and password to login to AWS console. Please be noted that access key and secret here cannot use for calling Amazon Web Service APIs due to some security issues, they are only used for submitting your GenAI.\n"
},
{
	"uri": "/advanced/genaiwidget/",
	"title": "How GenAI works on playground?",
	"tags": [],
	"description": "",
	"content": "1. GenAI on playground.digital.auto Developing SDV prototypes is not easy, as it requires knowledge and skills in different areas. But Generative AI can make it easier by taking care of some tasks, so developers can focus more on creativity. Generative AI is also part of the playground, which makes it more user-friendly for newcomers who may struggle with writing their first Python code, creating or choosing the right widget, or putting everything together to tell a story.\nThis diagram below will help you get started with your generative AI on the playground\nThe playground does not have any generative AI embedded in it. Instead, the playground functions as a bridge between developers and the LLMs hosted service.\nLLM Hosted Services\nDevelopers have the flexibility to utilize LLMs hosted services from various providers such as Microsoft Azure AI, Amazon Bedrock, or their own server infrastructure. Within these environments, developers are free to experiment, train, fine-tune, or instruct LLMs to align with the playground\u0026rsquo;s SDV Code (Python), Widget (HTML/CSS/JS), and Dashboard configuration (JSON).\nSubmit GenAI on the marketplace\nOnce developers have identified the most suitable model for their GenAI category, they can proceed to submit their LLMs to marketplace.digital.auto, providing deployment information including the endpoint URL, access key, and secret key.\nGenAI on the playground\nFollowing approval from the marketplace admin, the GenAI becomes accessible on the playground under the corresponding GenAI category: SDV ProtoPilot, Dashboard ProtoPilot, and WidgetProtoPilot. End-users can then utilize the developer\u0026rsquo;s GenAI to assist them in SDV prototyping seamlessly.\nEnd-User Interaction\nEnd users on the playground interact with the Generative AI by sending prompts through the provided deployment URL and credentials. The responses from the hosted LLM services are then rendered as outputs within the playground.\n2. Examples Before we dive into specific examples, let\u0026rsquo;s briefly explore how developers interact with models within the LLMs hosted services. This process involves training, fine-tuning, and instructing models using system messages, enabling developers to tailor them to their specific needs.\nIn the LLMs hosted services environment, developers have the flexibility to train, fine-tune, and instruct models through system messages. This approach allows models to learn and adapt to various tasks or scenarios based on the provided instructions and data.\nNow, let\u0026rsquo;s demonstrate the instruction model using system messages within the Amazon Bedrock and Microsoft Azure environments where developers can experiment with LLMs models.\nMicrosoft Azure AI\nAmazon Bedrock\nAfter experimenting with both LLMs models, the following system message can efficiently generate simple widgets:\nYou are expert web developer proficient in Tailwind, your task involves widget creation and code development.\rCoding Requirements:\r- RETURN THE FULL CODE\r- DO NOT ADD COMMENT AND YAPPING\r- Do not add comments in the code such as \u0026#34;\u0026lt;!-- Add other navigation links as needed --\u0026gt;\u0026#34; and \u0026#34;\u0026lt;!-- ... other news items ... --\u0026gt;\u0026#34; or in place of writing the full code. WRITE THE FULL CODE.\r- Repeat elements as needed. For example, if there are 15 items, the code should have 15 items. DO NOT LEAVE comments like \u0026#34;\u0026lt;!-- Repeat for each news item --\u0026gt;\u0026#34; or bad things will happen.\r- To integrate vehicle API, you MUST integrate the vehicle APIs with the following script structure within the code, below is the example:\r- User only give the prompt without specific ASSOCIATED_API so you will use these API for each scenarios:\r- If related to open/close driver door (value range: True/False, type: actuator): \u0026#34;Vehicle.Cabin.Door.Row1.Left.IsOpen\u0026#34;\r- If related to fan/hvac (value range: 0-100, type: actuator): \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed\u0026#34;\r- If related to set temperature of fan/hvac (value range: 16-30, type: actuator): \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.Temperature\u0026#34;\r- If related to open/close trunk (value range: True/False, type: actuator): \u0026#34;Vehicle.Body.Trunk.Rear.IsOpen\u0026#34;\r- If related to adjust driver seat position (value range: 0-10, type: actuator): \u0026#34;Vehicle.Cabin.Seat.Row1.Pos1.Position\u0026#34;\r- If related to turn on/off the low beam/light (value: True/False, type: actuator): \u0026#34;Vehicle.Body.Lights.IsLowBeamOn\u0026#34;\r- Example GET API Value:\r\u0026lt;script\u0026gt;\rlet ASSOCIATED_API = \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed\u0026#34;\rlet interval = null\rfunction onWidgetLoaded(options) {\rlet speedValue = document.getElementById(\u0026#34;speedValue\u0026#34;)\rlet fan_icon = document.querySelector(\u0026#39;.fan_icon\u0026#39;)\rinterval = setInterval(() =\u0026gt; {\rif(speedValue) {\rlet apiValueObject = getApiValue(ASSOCIATED_API)\rlet apiValue = apiValueObject?.value || 0\rspeedValue.innerText = apiValue\rlet duration = (-0.02 * apiValue) + 2.02\rif(!apiValue || apiValue === 0) {\rfan_icon.style.animationDuration = \u0026#34;0s\u0026#34;\r}\rif(apiValue || apiValue != 0){\rfan_icon.style.animationDuration = `${duration}s`\r}\r}\r}, 500) // Get API value every 0.5s\r}\rfunction onWidgetUnloaded(options) {\rif(interval) clearInterval(interval)\r}\r\u0026lt;/script\u0026gt;\r- Example SET API Value:\r\u0026lt;script\u0026gt;\rlet ASSOCIATED_API = \u0026#34;Vehicle.Exterior.AirTemperature\u0026#34;\r// elsewhere in the code set the temperature to associated_api\rsetApiValue(ASSOCIATED_API, currentData.current.temp_c);\r\u0026lt;/script\u0026gt;\r- The syncer will pass the sync the value between simulator and widget\rWidget Development:\r- Construct widgets that represent various car features through associated APIs.\r- Widgets should primarily use a dark mode theme, suitable for car in-vehicle infotainment (IVI) systems. Please use the Tailwind Neutral color palette (neutral 800-900-950)\r- Example Widgets: A fan widget displaying fan speed, and a speedometer widget showing vehicle speed.\r- Ease of Development: Design widgets that enable software engineers with minimal web development experience to easily create and integrate them with vehicle APIs.\r- This widget should be built using HTML, Tailwind CSS, and JavaScript, and must be fully responsive and centered to occupy the given space.\rLibraries and Resources:\r- Use this script to include Tailwind: \u0026lt;script src=\u0026#34;https://cdn.tailwindcss.com\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r- You can use Google Fonts\r- Font Awesome for icons: \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt;\r- Add the simulator via \u0026lt;script defer src=\u0026#34;https://bestudio.digitalauto.tech/project/BzR91b49OHqj/syncer.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\rOutput Format:\r- Return only the full code in \u0026lt;html\u0026gt;\u0026lt;/html\u0026gt; tags.\r- Do not include markdown \u0026#34;```\u0026#34; or \u0026#34;```html\u0026#34; at the start or end. With the prompt Create the fan widget that have a fan spinning 2.1 Amazon Bedrock Titan Express G1 Example Image of the widget generated by Amazon Bedrock Titan Express G1 \u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;script src=\u0026#34;https://cdn.tailwindcss.com\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\u0026#34;\u0026gt;\r\u0026lt;script defer src=\u0026#34;https://bestudio.digitalauto.tech/project/BzR91b49OHqj/syncer.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;style\u0026gt;\r.fan_icon {\ranimation: rotate infinite linear;\r}\r@keyframes rotate {\rfrom {\rtransform: rotate(0deg);\r}\rto {\rtransform: rotate(360deg);\r}\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body class=\u0026#34;bg-neutral-900 text-white flex items-center justify-center h-screen\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;w-64 h-64 bg-neutral-800 rounded-lg shadow-lg p-6 flex flex-col items-center justify-center\u0026#34;\u0026gt;\r\u0026lt;i class=\u0026#34;fas fa-fan fan_icon text-6xl mb-4\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\r\u0026lt;div class=\u0026#34;text-4xl font-bold\u0026#34; id=\u0026#34;speedValue\u0026#34;\u0026gt;0\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;script\u0026gt;\rlet ASSOCIATED_API = \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed\u0026#34;\rlet interval = null\rfunction onWidgetLoaded(options) {\rlet speedValue = document.getElementById(\u0026#34;speedValue\u0026#34;)\rlet fan_icon = document.querySelector(\u0026#39;.fan_icon\u0026#39;)\rinterval = setInterval(() =\u0026gt; {\rif(speedValue) {\rlet apiValueObject = getApiValue(ASSOCIATED_API)\rlet apiValue = apiValueObject?.value || 0\rspeedValue.innerText = apiValue\rlet duration = (-0.02 * apiValue) + 2.02\rif(!apiValue || apiValue === 0) {\rfan_icon.style.animationDuration = \u0026#34;0s\u0026#34;\r}\rif(apiValue || apiValue != 0){\rfan_icon.style.animationDuration = `${duration}s`\r}\r}\r}, 500) // Get API value every 0.5s\r}\rfunction onWidgetUnloaded(options) {\rif(interval) clearInterval(interval)\r}\r\u0026lt;/script\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt; 2.2 Microsoft Azure AI GPT-4 Example Image of the widget generated by Azure AI GPT-4 \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt;\r\u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt;\r\u0026lt;title\u0026gt;Fan Widget\u0026lt;/title\u0026gt;\r\u0026lt;script src=\u0026#34;https://cdn.tailwindcss.com\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt;\r\u0026lt;script defer src=\u0026#34;https://bestudio.digitalauto.tech/project/BzR91b49OHqj/syncer.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;style\u0026gt;\r.fan-icon-spin {\ranimation: spin infinite linear;\r}\r@keyframes spin {\r0% {\rtransform: rotate(0deg);\r}\r100% {\rtransform: rotate(360deg);\r}\r}\r\u0026lt;/style\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body class=\u0026#34;bg-neutral-900 min-h-screen flex items-center justify-center\u0026#34;\u0026gt;\r\u0026lt;div id=\u0026#34;fanWidget\u0026#34; class=\u0026#34;flex flex-col items-center justify-center text-white\u0026#34;\u0026gt;\r\u0026lt;div class=\u0026#34;fan_icon w-24 h-24 text-neutral-100\u0026#34;\u0026gt;\r\u0026lt;i class=\u0026#34;fas fa-fan fa-5x\u0026#34;\u0026gt;\u0026lt;/i\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;div class=\u0026#34;mt-4\u0026#34;\u0026gt;\r\u0026lt;span id=\u0026#34;speedValue\u0026#34; class=\u0026#34;text-4xl font-semibold\u0026#34;\u0026gt;0\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;text-xl\u0026#34;\u0026gt;RPM\u0026lt;/span\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;/div\u0026gt;\r\u0026lt;script\u0026gt;\rlet ASSOCIATED_API = \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed\u0026#34;\rlet interval = null\rfunction onWidgetLoaded(options) {\rlet speedValue = document.getElementById(\u0026#34;speedValue\u0026#34;)\rlet fanIcon = document.querySelector(\u0026#39;.fan_icon i\u0026#39;)\rinterval = setInterval(() =\u0026gt; {\rlet apiValueObject = getApiValue(ASSOCIATED_API)\rlet apiValue = apiValueObject?.value || 0\rspeedValue.innerText = apiValue\rif(apiValue || apiValue !== 0) {\rfanIcon.classList.add(\u0026#39;fan-icon-spin\u0026#39;)\rlet duration = 2 - (apiValue / 50) // Adjust fan speed animation based on API value\rfanIcon.style.animationDuration = `${duration}s`\r} else {\rfanIcon.classList.remove(\u0026#39;fan-icon-spin\u0026#39;)\r}\r}, 500)\r}\rfunction onWidgetUnloaded(options) {\rif(interval) clearInterval(interval)\r}\r// This function is called on window load to start the widget\rwindow.onload = function () {\ronWidgetLoaded();\r};\r// This function is called before the window is closed/unloaded\rwindow.onbeforeunload = function () {\ronWidgetUnloaded();\r};\r\u0026lt;/script\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt; "
},
{
	"uri": "/campaign/gen-ai-awards-2024/3-access-aws-bedrock/",
	"title": "Access AWS Bedrock console",
	"tags": [],
	"description": "",
	"content": "This guide show how you can access Amazon Web Service Bedrock console after you applied for credits and approved\nFrom previous Review your team screen, copy your IAM user and IAM password, then click Open AWS Bedrock. Fill your IAM username and password. Then click Sign in. Search for AWS Bedrock in search bar, then click AWS Bedrock. Click get started to navigate to Bedrock console. Change your AWS region to US East (N. Virginia) - us-east-1. This ensures that you can access all available LLM models. "
},
{
	"uri": "/campaign/gen-ai-awards-2024/4-build-first-genai/",
	"title": "Build your GenAI",
	"tags": [],
	"description": "",
	"content": "This guide shows how you can build a simple GenAI Python by using prompt engineering methods.\n1. After accessing the AWS Bedrock console, navigate directly to the Playground/chat section to begin experimenting. 2. Select the model you want to experiment with. 3. First, choose the provider, then select the model from this provider, and press \u0026ldquo;apply\u0026rdquo; to return to the playground chat interface. 4. Try the simple prompt: \u0026ldquo;Generate an SDV Python code to open the driver\u0026rsquo;s door.\u0026rdquo; 5. The returned result is general and not specific to the playground. The LLMs generate too many comments, whereas we only require code. 6. You can enhance model responses by employing specific instructions known as \u0026ldquo;system messages\u0026rdquo;. These messages provide context for the model during API calls and are hidden from the end user, helping the model understand the context and follow your directives more accurately. Below are examples of system messages that you can use as guidelines for constructing your own instructions:\nGenAI Python: (https://bewebstudio.digitalauto.tech/data/projects/alSxlS1Qkf20/GenAI_Python.md)\nGenAI Dashboard: (https://bewebstudio.digitalauto.tech/data/projects/alSxlS1Qkf20/GenAI_Dashboard.md)\nGenAI Widget: (https://bewebstudio.digitalauto.tech/data/projects/alSxlS1Qkf20/GenAI_Widget.md)\nDo not copy these system messages directly into the interface. Instead, use them as templates to create specific instructions under \u0026ldquo;Instruction:\u0026rdquo; and place your requests under \u0026ldquo;Requirement:\u0026rdquo; to tailor the model\u0026rsquo;s output to your needs. After identifying the most effective instruction or system message, save it and submit it to marketplace.digitalauto.tech for public use.\n7. The returned result after adding the instruction will include only code specific for the playground without any additional comments. 8. Paste the code into the code tab, then run the prototype and observe the API value changes. Optionally, consider adding widgets to better visualize the API\u0026rsquo;s value. If you are not yet familiar with the Playground, it is advisable to build your first prototype by following this \u0026ldquo;Hello world\u0026rdquo; prototype\u0026rsquo;s guide\n"
},
{
	"uri": "/dreamkit/overview/",
	"title": "Overview dreamKIT",
	"tags": [],
	"description": "",
	"content": "The digital.auto dreamKIT is a proof-of-concept (PoC) hardware, providing a hands-on, physical experience for SDV applications. With the dreamKIT you can try out your digitally developed SDV features on a physical device. Therefore, it allows you to transfer your use case from the virtual exploration phase into the productization phase.\ndreamKIT is a candidate for SDV Alliance Integration Blueprint. Check it out the latest COVESA white paper release here\nShould you have any specific interest on dreamKIT, please give us feedback here https://forms.office.com/e/BiJcMEhbBy\nThe digital.auto dreamKIT includes a vehicle computing unit, a central gateway, and a mock-up in-vehicle infotainment touch screen. dreamKIT has CAN and Ethernet interfaces to connect with external devices, that can form a Zonal E/E Architecture network topology.\nThe dreamKIT is a proven PoC device, which is used in multiple international show cases, as well as in different co-innovation challenges for software engineers.\nFeature overview for development, experimentation, and innovation:\nSeamless integration with playground.digital.auto: SDV applications developed on playground could be deployed to the dreamKIT within seconds, wirelessly using socketIO technology. Built in SDV runtime environment: powered by Eclipse SDV solutions. Ease of customizing and experimenting a zonal EE architecture: dreamKIT has built-in central gateway – which has CAN/ CANFD and Ethernet interfaces - to connect and expand the network topology to different zone controller ECUs Ease of connecting an AUTOSAR embedded ECU: dreamKIT has a SDV runtime environment, that with minimal configuration (via OTA) could recognize and integrate with an external device. "
},
{
	"uri": "/campaign/gen-ai-awards-2024/",
	"title": "GenAI Awards 2024",
	"tags": [],
	"description": "",
	"content": "Learn how to enroll and submit to GenAI Awards 2024 Competition\n"
},
{
	"uri": "/campaign/gen-ai-awards-2024/5-submit-your-genai/",
	"title": "Submit your genAI",
	"tags": [],
	"description": "",
	"content": "This guide explains how to submit your work for the GenAI Awards. You can make your submission through the Marketplace, which accepts a variety of entries, including GenAI, Widget, and Vehicle App projects.\nOn the Marketplace website, we refer to a GenAI submission as a \u0026ldquo;Package.\u0026rdquo;\nFrom previous Review your team screen. Click Go to marketplace, or you can go to https://marketplace.digital.auto In the marketplace, if you\u0026rsquo;ve previously logged into the GenAI website, your login should carry over. If you find yourself not signed in, please log in again using the same account with which you registered your team on the GenAI website.\nClick Add package to create a new package. Fill the information\n4.1 Basic information. This includes Package Name, Short Description, Detail Description. The Visibility should be public 4.2 Choose a category. In the category field, select GenAI. Next, select the type of GenAI that corresponds to your work. A small alert will pop up to remind you that you are part of a team, and this submission will be entered into the GenAI Awards Competition. 4.3 Fill credentials and config from Review your team screen in previous step. For the message, please refer to How to build your GenAI step.\nFor endpointURL:\nThe syntax of endpoint URL is: https://bedrock-runtime.us-east-1.amazonaws.com/model/\u0026lt;model_id\u0026gt;/invoke-with-response-stream Refer to this AWS document to find your model_id: https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns For example, if you use Claude 3 Sonnet model, your endpointURL is: https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke-with-response-stream 4.4 Add images. Package Icon, Package Cover, and Feature Images are all required. For Feature Images, you need at least one image. Click Submit to submit your GenAI.\nTo view all your submissions, click My Package tab in the header of the website. Voila, you have submitted your GenAI. Next step, test it out at our playground website: https://digitalauto.netlify.app\nNote: To submit another genAI, just start over from step 1 again "
},
{
	"uri": "/basics/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Getting Started Walk you through an end to end SdV applications creation and experimentation journey.\n"
},
{
	"uri": "/dreamkit/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "\n"
},
{
	"uri": "/campaign/gen-ai-awards-2024/6-update-genai/",
	"title": "Update your submission",
	"tags": [],
	"description": "",
	"content": "This guide shows how you can update your GenAI submission\n1. After accessing the marketplace, navigate to \u0026ldquo;My package\u0026rdquo; 2. Select your GenAI Awards submission pacakge 3. Click update button 4. Adjust the deployment URL, dredentials, and instruction "
},
{
	"uri": "/advanced/socket-io-provider/",
	"title": "Socket.IO Integration",
	"tags": [],
	"description": "",
	"content": "Connect digital.auto playground to third party system via socket.io\nThere is many ways to connect to digital.auto from 3rd systems/apps such as HTTP API, SOAP, etc., however those methods are just one-way request. To support you fetch continuous data, 2-ways communication, we build up some socker.io servers below:\nEU: https://bridge.digitalauto.tech Asia: https://bridge.digitalauto.asia (our server run socket.io version 4.x, no auth require)\nFor more infomation, visit there site: https://socket.io/\nHow communication work Let\u0026rsquo;s get started 1. Create new plugin, and add below code Step 01: Create an empty plugin const loadScript = (boxWindow, url) =\u0026gt; { return new Promise(async (resolve, reject) =\u0026gt; { try { const script = boxWindow.document.createElement(\u0026#34;script\u0026#34;); script.defer = true; script.referrerPolicy = \u0026#34;origin\u0026#34; script.src = url; boxWindow.document.head.appendChild(script); script.addEventListener(\u0026#34;load\u0026#34;, () =\u0026gt; resolve(undefined)); } catch (e) { reject(); } }); } const PROVIDER_ID = \u0026#34;PYTHON-CLIENT-SAMPLE\u0026#34; const plugin = ({widgets, simulator, vehicle}) =\u0026gt; { widgets.register(\u0026#34;Client\u0026#34;, async (box) =\u0026gt; { }) return {} } Step 02: Load the socket.IO client await loadScript(box.window, `https://cdn.socket.io/4.6.0/socket.io.min.js`) Step 03: connect to socket IO server, and register to the provider you want to communicate const socket = box.window.io(\u0026#34;https://bridge.digitalauto.tech\u0026#34;); const onConnected = () =\u0026gt; { console.log(\u0026#34;Io connected\u0026#34;) socket.emit(\u0026#34;register_client\u0026#34;, { master_provider_id: PROVIDER_ID }) } socket.on(\u0026#34;connect\u0026#34;, onConnected); From now on, when provider sends some data, you can get it. Now we define function to handle message from provider.\nStep 04: Handle incoming data from provider const messageFromProvider = (payload) =\u0026gt; { if(payload.cmd == \u0026#39;showSpeed\u0026#39;) { lblSpeed.innerText = payload.data } } socket.on(\u0026#39;message_from_provider\u0026#39;, messageFromProvider) On provider side, when provider calls socket.emit(\u0026ldquo;send_to_all_clients\u0026rdquo;, payload) then the function messageFromProvider will be called with the payload data\nStep 05: Send a request to Provider socket.emit(\u0026#34;request_provider\u0026#34;, { to_provider_id: PROVIDER_ID, cmd: \u0026#34;Start\u0026#34;, // cmd send to provider data: 1 // data for the cmd, data may be number, string, object or null depend on cmd }) // now server will handle that cmd and reply to you (only reply to the requester, not other clients) // we add a function to handle data reply from provider const messageFromProvider = (payload) =\u0026gt; { if(payload.cmd == \u0026#39;showSpeed\u0026#39;) { lblSpeed.innerText = payload.data } } socket.on(\u0026#39;provider_reply\u0026#39;, onProviderReply) You have the free to define cmd set and data. On your provider code, handle commands and reply to client.\nFull sample plugin code available here: https://media.digitalauto.tech/data/files/668377ef-41a9-4b36-af80-2ef4e4fc6666TestIO.js\n2. Create a provider to answer to client request Sample python PROVIDER here https://github.com/nhan-orgs/python_provider Sample javascript PROVIDER here https://github.com/nhan-orgs/javascript_provider For other languages, please visit Socket.IO client document for sample code. 3. Build a prototype to test the above code We have sample prototype at this link: https://digitalauto.netlify.app/model/1jNrd7yS32ZzFo3WXrJy/library/prototype/3qgZS366ARQaCkPEBtuk/view/run\nDashboard config:\n[ { \u0026#34;boxes\u0026#34;: [1,2], \u0026#34;plugin\u0026#34;: \u0026#34;SocketIO\u0026#34;, \u0026#34;widget\u0026#34;: \u0026#34;Client\u0026#34; } ] "
},
{
	"uri": "/engaged/vss_basic/",
	"title": "Vehicle API",
	"tags": [],
	"description": "",
	"content": "Vehicle API introduction Vehicle API is an abstract way to manipulate everything an actuator on vehicle, or get a sensor value from vehicle without knowledge of ECU and E/E Architechture. On the context of Software define Vehicle, we are an application developer. We would like to make an smart application to serve special purpose. Let\u0026rsquo;s say we would have an AI algorithm to turn on or off the headlight depend on weather condition and some other factors, and then the algorithm output say that we should turn the light on. How can we do that if we don\u0026rsquo;t know anything about embedded, CAN signal, ECU, gateway, and plenty of security, safety stuff\u0026hellip; you can not do that. Vehicle API was born to solve that problem.\nThere are plenty of Vehicle API depend of OEM, Vehcile Model,\u0026hellip; and definitely each one have different syntax, params and structure. digital.auto dealing with problem by allow you below approach:\nAllow you select an exist API set Allow you extend an exist API set Allow you to define your own API set One again, digital.auto is a playground to dev and test your vehicle application. It is very important to using the APIs compatible with the exist APIs on the vehicle. So that, after successfully testing your vehicle, you can deploy your app directly to your vehicle and run on it.\nTo make thing simple and general, our examples will go with COVESA VSS API, an open source standard that growing and promissing to be industrial standard in the near future.\nCOVESA VSS API The Vehicle Signal Specification (VSS) is an initiative by COVESA to define a syntax and a catalog for vehicle signals. The source code and releases can be found in the VSS github repository. Some tools for parsing and converting VSS files can be found in the VSS-tools github repository.\nCOVESA VSS nesting API together in a tree When you create new Vehicle Model, make sure you choose the correct API standard. Now it\u0026rsquo;s time to explorer API tree. Select a Vechicle Model, than click on Vehicle APIs. In the Vehicle APIs view, your using search box to looking for the API On the left side, we have list of API. Each API will belong to one os these type:\nBranch: a tree node contain other APIs or branchs Actuator: API to control a actuator Sensor: API to get value from a sensor Attribute: acceptable value for Actuator or Sensor Using the search box to search for API name, you can specify the only types you want to search.\nOn top right corner, click on \u0026ldquo;Tree View\u0026rdquo; to render the API list in Tree style.\nWishlist API In case you need to use an API, but it does not exist. You can add a \u0026ldquo;wish API\u0026rdquo;.The wishlist API is marked with a purpel cicle W icon. You can also filter default API or WishList API from the search box.\nAdd new Wistlist API Pay attention on the API name, the syntax must follow the rule bellow.\n[branch_name].[branch_name].[\u0026hellip;].name\nIf your top most branch name is Vehicle, you always have to start your api with Vehicle.\nUsing API Up to now you have plenty of API, defaults API set define by standard and Wishlist API define by you. Remember that all of them associate with one Vehicle Model. All Protoype under a Vehicle Model share that API set.\nIn prototype, when writing python code on the left pannel, you can search and click API on the right panel.\nFor Actuator, we have Set/Get/Subcrible functions, but for Sensor, we only can Get/Subcrible\n"
},
{
	"uri": "/engaged/widget_basic/",
	"title": "Using builtin widget",
	"tags": [],
	"description": "",
	"content": "At the \u0026ldquo;Create \u0026lsquo;Hello World\u0026rsquo; Prototype\u0026rdquo; section, you already now how to pick a widget from gallery and put it to dashboars. In this section, we will go deeper on each kind of widget, change the option to she how we can manipulate them just by some simple arguments.\nBefore you begin, let make a simple prototype with below python code. If you don\u0026rsquo;t know how, please read this tutorial first \u0026ldquo;Create \u0026lsquo;Hello World\u0026rsquo; Prototype\u0026rdquo; section\nfrom sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() for i in range(10): # code to turn headlight on await vehicle.Body.Lights.IsLowBeamOn.set(True) await aio.sleep(1) # code to turn headlight on await vehicle.Body.Lights.IsLowBeamOn.set(False) await aio.sleep(1) Very simple code, blinking headlight on/off every second ten times. Now let\u0026rsquo;s open Dashboars_Config, click \u0026ldquo;Pick A Widget from Gallery\u0026rdquo; and review the list of Widget we have:\nSingle-API-Widget: show value of a single API, very simple widget to check you API value Table-APIs-Widget: a table show value of a list of API Chart-APIs-Widget: a simple chart, can be in bar or line, Image-By-VSS-Value: each API value mapping to an image, image will change base on api value Wiper-Simultor: an wiper animation simulator, when you set wiper mode, the animation with reflect the state of wiper GoogleMap-Direction: select a start point and end point, and base on your GPS API, the car will move on the maps Embedded-Widget: using iframe to embedded an external site, may use with our Web Studio to edit the external site The rest of this section will show you how to config above widgets:\nSingle-API-Widget This is them simplest widget to getting started. The widget will only show realtime value of an API. This is the options you can alter.\nThe option string should be an valid json string\nlabel: The widget provide two boxes: the upper for value text and the bottom for label text. If label is empty, it will be completely remove, give all space for value text. api: Vehicle API you want to show value labelStyle: css style string for the label box valueStyle: css style string for the value box boxStyle: css style string for the parent box(the whole widget box) Let make a simple example, change the options to below string:\n{ \u0026#34;label\u0026#34;: \u0026#34;Vehicle Speed (km/h)\u0026#34;, \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Speed\u0026#34;, \u0026#34;labelStyle\u0026#34;: \u0026#34;color:teal;font-size:22px\u0026#34;, \u0026#34;valueStyle\u0026#34;: \u0026#34;color:orange;font-size:80px;font-weight:700;\u0026#34;, \u0026#34;boxStyle\u0026#34;: \u0026#34;background-color:white;\u0026#34; } The we have the result like below\nTable-APIs-Widget A simple table show APIs current value. You just need to provide the list of api you want to monitor in the options string:\n{ \u0026#34;apis\u0026#34;: [ \u0026#34;Vehicle.Body.Lights.IsLowBeamOn\u0026#34;, \u0026#34;Vehicle.Speed\u0026#34; ] } Below image show how the table render with above options:\nChart-APIs-Widget Simple line or bar chart, show recent value of some APIs\nLet try with this config:\n{ \u0026#34;chartType\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;num_of_keep_item\u0026#34;: 30, \u0026#34;chart_tick\u0026#34;: 500, \u0026#34;signals\u0026#34;: [ { \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.Temperature\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34; }, { \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Cabin.HVAC.Station.Row1.Left.FanSpeed\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;orange\u0026#34; } ] } We monitoring the value of Row1.Left.Temperature and Row1.Left.FanSpeed, for each API, you can specify color.\nchartType: could be line or bar chart_tick: mean how frequenly a new value be recorded and add to chart, unit is milisecond. num_of_keep_item: maximum value record be keep in chart The chart will show data of the last num_of_keep_item*chart_tick milisecond. With above setting, we will have result as below: Image-By-VSS-Value We ussually using image to demostate the state of something, this widget could be useful for that case. The widget will continuously read API value, and each value mapping to an image, image will change base on API value\nLet try with this config:\n{ \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Body.Lights.IsLowBeamOn\u0026#34;, \u0026#34;valueMaps\u0026#34;: [ { \u0026#34;value\u0026#34;: false, \u0026#34;img\u0026#34;: \u0026#34;https://bestudio.digitalauto.tech/project/Ml2Sc9TYoOHc/light_off.png\u0026#34; }, { \u0026#34;value\u0026#34;: true, \u0026#34;img\u0026#34;: \u0026#34;https://bestudio.digitalauto.tech/project/Ml2Sc9TYoOHc/light_on.png\u0026#34; } ] } Very simple, when LowBeam value if true, we display light_on.png, else we display light_on.png The widget is not limit for boolean, you can use with API return interger value\nWiper-Simulator This widget is quite solid, no need to input option. The widget alway linking with API: Vehicle.Body.Windshield.Front.Wiping.Mode Let try some code and config as image below, then run the Dashboard, you will see the config wiper run 5s then off.\nThis is the result:\nGoogleMap-Direction This widget render maps in direction mode, you can specify start point and end point. Thenm when the app running, position of your can will read from posision API and render you car on maps. In option, you also can specify the url of the image for the car icon you want.\n{ \u0026#34;directions\u0026#34;: [ { \u0026#34;lat\u0026#34;: 48.149497, \u0026#34;lng\u0026#34;: 11.523194 }, { \u0026#34;lat\u0026#34;: 50.445168, \u0026#34;lng\u0026#34;: 11.020569 } ], \u0026#34;icon\u0026#34;: \u0026#34;\u0026#34; } And here is how it look like\nEmbedded-Widget Indeed, this widget need you provile url of another website, then if will using iframe to embedded that site to the widget box. The only require option is url of the external site. Let\u0026rsquo;s try to embedded an Youtube video to our playground.\n{ \u0026#34;url\u0026#34;: \u0026#34;https://www.youtube.com/embed/ypR2cpdh6JA?si=S1jxtr4g1wXeZWQk\u0026#34; } And here is the result\nEmbedded-Widget is the most powerful widget, digital.auto come up with a Web Studio at the address https://studio.digitalauto.tech With the WebStudio, you can write some HTML/CSS/JS code to build up a beautify widget and \u0026rsquo;embbeded\u0026rsquo; it to the playground. Follow the instruction here to see how to create your own widget and resuse it on multiple prototype.\nHappy coding!\n"
},
{
	"uri": "/basics/overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "1. Overview The open and web based digital.auto playground offers a rapid prototyping environment to explore and validate ideas of a Vehicle App.\ndigital.auto interacts with different vehicle sensors and actuators via standardized APIs specified by the COVESA Vehicle Signal Specification (VSS) without custom setup requirements.\nWithin the platform you can:\nbrowse, navigate and enhance vehicle signals (sensors, actuators and branches) in the Vehicle API Catalogue mapped to a 3D model of the vehicle. build Vehicle App prototypes in the browser using Python and the Vehicle API Catalogue. test the Vehicle App prototype in a dashboard with 3D animation for API calls. create new plugins, which usually represent UX widgets or remote server communication to enhance the vehicle mockup experience in the playground. collect and evaluate user feedback to prioritize your development portfolio. 2. Start the journey of a Vehicle App As first step open digital.auto, select Get Started in the prototyping section of the landing page and use the Vehicle Model of your choice.\nYou now have the possibility to browse existing vehicle signals for the selected vehicle model which you can use for prototyping your Vehicle App by clicking on Vehicle APIs.\n2.1. Add additional Vehicle APIs Note For this feature, a digital.auto account is required. Move on to the Login page to learn how to simply create an account for the playground. If the ideation of your Vehicle App prototype comes with any new Vehicle API which is not part of the standard VSS you also have the option to include it into your pre-selected model by clicking the + New Wishlist API button. After filling out all required fields, simply click the create button - this will commit the new API to the existing model.\n2.2. Prototype an idea of a Vehicle App The next step would be to prototype your idea. To do so:\nClick on Prototype Library of your selected model. Create a new prototype, by clicking on New Prototype and filling out the information or select one from the list. Click on the Open button. Go to the Code section and start your prototype right away. 2.3. Test the prototype of a Vehicle App Testing of your prototype starts in the Run section.\nYou will find a dashboard consisting all vehicle and application components similar to mockups.\nThe control center on the right side has an integrated terminal showing all of your prototyped outputs as well as a list of all called VSS API’s.\nThe Run button executes all your prototype code from top to bottom. The Debug button allows you to step through your prototype line by line.\nTo get started quickly, the digital.auto team has added a number of widgets to simulate related elements of the vehicle – like doors, seats, light, etc. – and made them available in the playground.\n"
},
{
	"uri": "/engaged/",
	"title": "Build Your First Prototype",
	"tags": [],
	"description": "",
	"content": "Chapter 2 Build Your First Prototype Sounds interesting enough? Then let\u0026rsquo;s get our hands dirty on the playground.\n"
},
{
	"uri": "/campaign/",
	"title": "Campaign",
	"tags": [],
	"description": "",
	"content": "Chapter 3 Campaign Contains information of all campaigns and competitions of digital.auto\n"
},
{
	"uri": "/campaign/gen-ai-awards-2024/7-use-genai/",
	"title": "Use GenAI on playground",
	"tags": [],
	"description": "",
	"content": "1. SDV ProtoPilot (GenAI Python) 1. Click on the SDV Protopilot button. Then enter your input prompt and choose a generator. 2. Select a generator from the marketplace, your own generator will be displayed here after the admin approves it. 3. Input prompt and selected a generator, click on the \u0026ldquo;Generate\u0026rdquo; button and wait for the code to be generated. 4. This may take from 30 to 90 seconds, depending on the complexity and quality of the generator. If the generator is well-trained and fine-tuned, it will produce the code faster and more accurately. Click on the \u0026ldquo;Add new generated code\u0026rdquo; if you satisfy with the new generated code. 2. Dashboard ProtoPilot (GenAI Dashboard) After you have generated the SDV Python code, you may want to visualize it and tell a story with it. For that, you can use the Dashboard Protopilot feature, which will help you pick and compose multiple widgets to create a dashboard for your code. The Dashboard Protopilot can use the built-in widgets in the model or the ones from the marketplace depend on how the model\u0026rsquo;s authors train/fine-tune/config it.\n1. Go to the dashboard config tab and click the \u0026ldquo;Dashboard Protopilot\u0026rdquo; button. 2. Select a generator from the marketplace, your own generator will be displayed here after the admin approves it. 3. Input prompt and selected a generator, click on the \u0026ldquo;Generate\u0026rdquo; button and wait for the code to be generated. 4. When the dashboard is generated, you will see it on the right hand side of the popup window. You can also view the raw configuration by clicking the \u0026ldquo;View Dashboard\u0026rdquo; button Click on the \u0026ldquo;Add new dashboard coonfig\u0026rdquo; if you satisfy with the new generated config. 3. Widget Protopilot (GenAI Widget) If you have generated the SDV Python code, but you cannot find a suitable widget to visualize it in the built-in ones or the ones from the marketplace, you may want to create your own widget with the Widget Protopilot feature. This feature will help you generate a custom widget based on your input prompt. The procedure is similar to the Dashboard Protopilot and the SDV Protopilot, but with some differences. Here is how it works:\n1. Go to the dashboard config tab and add a widget. This will open a popup where you can add a widget by \u0026ldquo;Widget Protopilot\u0026rdquo;. 2. Click on the \u0026ldquo;Widget Protopilot\u0026rdquo; tab to open the Widget Protopilot popup window. 3. Input prompt and selected a generator, click on the \u0026ldquo;Generate\u0026rdquo; button and wait for the code to be generated. 4. Select a generator from the marketplace, your own generator will be displayed here after the admin approves it. 5. This may take from 30 to 90 seconds, depending on the complexity and quality of the generator. If the generator is well-trained and fine-tuned, it will produce the widget faster and more accurately. 6. When the widget is generated, you will see it on the right hand side of the popup window. The widget will be embedded into an iframe for you to preview. You can see how the widget looks and works, and if it meets your expectations and requirements. If you are satisfied with the widget, you can click on the \u0026ldquo;Add Widget\u0026rdquo; button at the bottom of the right hand side. This will add the widget to your selected dashboard box on the playground, where you can edit it and see the output. After you have added the widget to your dashboard box, you may want to view or modify the code of the widget. You can do this by hovering over the widget box and clicking on the icon that links to the Widget Studio. The Widget Studio is a tool that allows you to edit the HTML, CSS, and JavaScript code of the widget. You can use the Widget Studio to customize the widget to suit your application or debug it if the GenAI made some errors. The widget code is more complex than the SDV Python code, so the GenAI may not always generate it perfectly. You can use the Widget Studio to fix any issues or improve the widget as you wish.\n4. How playground handling GenAI responses When interacting with a GenAI model through playground, it\u0026rsquo;s important that the responses from the model servers are structured in a way that allows our system to effectively process and present the output, regardless of the hosting environment, be it Amazon Bedrock or another infrastructure.\nFor secure and structured communication, our platform uses the HTTP POST method to send requests to your model\u0026rsquo;s endpoint, which includes user input prompts and, if applicable, system messages.\nModels Hosted on Amazon Bedrock: For models on Amazon Bedrock, the platform sends a structured request containing the user input, which is then expected to return a response in a specific JSON format. The request includes necessary AWS credentials and the input prompt, as shown below:\n{ \u0026#34;url\u0026#34;: \u0026#34;\u0026lt;Your Model\u0026#39;s Endpoint URL\u0026gt;\u0026#34;, \u0026#34;accessKey\u0026#34;: \u0026#34;\u0026lt;Your AWS Access Key\u0026gt;\u0026#34;, \u0026#34;secretKey\u0026#34;: \u0026#34;\u0026lt;Your AWS Secret Key\u0026gt;\u0026#34;, \u0026#34;system\u0026#34;: \u0026#34;\u0026lt;Your system message\u0026gt;\u0026#34;, \u0026#34;input\u0026#34;: \u0026#34;\u0026lt;User input prompt\u0026gt;\u0026#34; } The model\u0026rsquo;s response vary depend on which model, the examples below is from the AWS Amazon Bedrock Titan Express G1\n{ \u0026#34;results\u0026#34;: [ { \u0026#34;outputText\u0026#34;: \u0026#34;This is the extracted output for AWS Amazon Bedrock Titan Express G1\u0026#34; } ] } or Anthropic Claude 3\n{ \u0026#34;content\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;This is the extracted output for AWS Amazon Anthropic Claude 3\u0026#34; } ] } Models Hosted on other infrastructures the request includes the user input within a messages array, which may also contain initial system messages. The request is properly formatted with the necessary content type headers and authentication details:\n{ \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;Your system message\u0026gt;\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;\u0026lt;User input prompt\u0026gt;\u0026#34; } ] } The expected response format for these models is as follows:\n{ \u0026#34;choices\u0026#34;: [ { \u0026#34;message\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;This is the extracted output for models hosted on other infrastructures.\u0026#34; } } ] } To clarify, here are examples illustrating how your model\u0026rsquo;s responses should be formatted for extraction across various GenAI features:\nSDV ProtoPilot response should include only python format\nfrom sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() async def toggle_low_beam(): # Init the default value await vehicle.Body.Lights.IsLowBeamOn.set(False) await aio.sleep(1) for i in range(0, 10): # Get the current state of low beam low_beam_state = await vehicle.Body.Lights.IsLowBeamOn.get() # Toggle the low beam await vehicle.Body.Lights.IsLowBeamOn.set(not low_beam_state) await aio.sleep(1) await toggle_low_beam() Dashboard ProtoPilot Example (expected in JSON format):\n{ \u0026#34;auto_run\u0026#34;: false, \u0026#34;widgets\u0026#34;: [ { \u0026#34;plugin\u0026#34;: \u0026#34;Builtin\u0026#34;, \u0026#34;widget\u0026#34;: \u0026#34;Chart-APIs-Widget\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;chartType\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;num_of_keep_item\u0026#34;: 30, \u0026#34;chart_tick\u0026#34;: 500, \u0026#34;signals\u0026#34;: [ { \u0026#34;api\u0026#34;: \u0026#34;Vehicle.Proximity\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#3b82f6\u0026#34; } ] }, \u0026#34;boxes\u0026#34;: [4, 5] } ] } Widget ProtoPilot Example (should start with an HTML tag):\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;!-- HTML Head content with meta, styles, and scripts --\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;h-screen grid place-items-center bg-slate-100 select-none\u0026#34;\u0026gt; \u0026lt;!-- Body content with widget structure and functionality --\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; To ensure the integration goes smoothly, it is advised to double-check the format of your model\u0026rsquo;s responses before integrating them with the playground. This preparation can help prevent potential issues and make sure your model\u0026rsquo;s functionalities are effectively utilized within the playground.\nConclusion We hope this guide has been helpful for you to learn how to submit and use your generative AI on the playground. With this tool, you can create amazing SDV prototypes with less effort and more creativity. You can also share your models and widgets with other users on the marketplace, and use the ones that they have created and submitted. The playground is a place where you can experiment, explore, and have fun with generative AI.\nWalkthrough videos Experiment GenAI on playground: SDV Code \u0026amp; Dashboard Experiment GenAI on playground: SDV Code \u0026amp; Widget "
},
{
	"uri": "/advanced/playground_n_soafee/",
	"title": "Cloud native experiment with playground",
	"tags": [],
	"description": "",
	"content": "1. Introduction Typically the process to develop, build, test and deploy one vehicle applications takes months or years, at a very high cost. Due to the complexity of development process and the limitation of the development tools which are not easy and efficient to use.\nOn the trending of software defined vehicle, the software development process is changing. The software development process is moving from traditional waterfall model to agile model. The software development tools are moving from traditional IDE to cloud based IDE. The software development process is moving from traditional manual process to CI/CD process.\nThe dependency on hardware in the loop (HIL) is also changing. The HIL is moving from traditional HIL to virtual HIL. The HIL is moving from traditional hardware to cloud based hardware. The HIL is moving from traditional manual process to CI/CD process. By using the virtual HIL, the development cost time to market can be reduced significantly. To make this achievement, we need to shift more test from HIL to SIL. The SIL test is moving from traditional SIL to cloud based SIL. The SIL test is moving from traditional manual process to CI/CD process. However most the traditional SIL test run on x86 microprocessor. But the vehicle ECU/VCU is running ARM. It\u0026rsquo;s mean that, the SIL test is not running on the same architecture as the vehicle ECU/VCU. It\u0026rsquo;s not good for the SIL test. The SIL test should run on the same architecture as the vehicle ECU/VCU. The SIL test require an ARM based cloud. With the support of AWS Graviton, we can run the SIL test on ARM based cloud. The SIL test can run on the same architecture as the vehicle ECU/VCU.\nWill AWS Graviton we can build an infrastructure on cloud the same as the one on vehicle. Next need a standard methodology to develop, build, test and deploy the vehicle application on cloud. This is where digital.auto and SOAFEE come in. We use digital.auto playground as an IDE to development and quickly test the app idea on web browser. We use EWAOL (an implementation of SOAFEE) to setup the infrastructure on AWS Graviton. Then we can test the app on a environment the same as the one on vehicle. Finally, we deploy again the app on dreamKIT, a hardware kit provide the full hardware stack for a vehicle to test the app with real physical signal.\n2. Setup Because the scope of this document is only showcasing, the detail of step by step environment setup on both AWS Graviton and dreamKIT will be available outside of this page.\nhere is a brief view on the existing EWAOL setup on AWS Graviton at a Virtual Machine name: VM-Graviton-9b9c1d6c\nDetail steps will be provided in another document.\nWithin this VM, we have: - EWAOL: a SOAFEE implementation - docker and dapr: to run the app - Eclipse Kuksa Data Broker - Eclipse Velocitas SDK - dk-manager: a small app write in C++ to receive deploy message from digital.auto playground\n3. Showcasing process Step 1: Write and experiment SDV app on digital.auto playground (Chrome) Step 2: Test sdv app on AWS Graviton Step 3: Test sdv app on HW PoC dreamKIT Step 1: Write and experiment SDV app on digital.auto playground We use digital.auto playground as an IDE to development and quickly develope and test the SdV app idea on web browser. The app is written in python follow Eclipse Velocitas Python template. By using COVESA VSS API, we can easily access the vehicle signal and control the vehicle actuator. The app is running on Chrome browser just for demonstration purpose.\ndigital.auto playground provide a dashboard to test the outcome of the app.\nWrite a simple headlight test app as below: Pick a api Image-By-VSS-Value widget to test the app. The widget will show the image of the headlight status base on the API value.\nAnd then test the app on dashboard The image widget and the out come form terminal show that the app is running as expect. But the test here is not fully functional test. We need to test the app on a complete vehicle environment. That\u0026rsquo;s why we need to deploy the app on EWAOL. Next, we will deploy the app on a AWS Graviton VM.\nStep 2: Test sdv app on AWS Graviton Back to the tab code, click on Deploy button. Then pick VirtualMachine icon and pick VM-Graviton-9b9c1d6c from the list. Finally, click deploy. The app will be sent to Graviton VM.\nNow it time to run the app on VM. Go to runtime manager at this link: https://digitalauto.netlify.app/runtime-manager\nAt top right corner, pick VM-Graviton-9b9c1d6c from the target list. Then click on the app name to start the app. For this demo, we run the app name \u0026ldquo;Blink head light\u0026rdquo;. For your case, the app name is the prototype name you made on digtial.auto playground.\nAfter the app is running, we can see get the log of the app by click \u0026ldquo;Load-logs\u0026rdquo; for the app you want to see the log (the one you run)\nThe log with the timestamp show that the app is running well on Graviton VM. Finally, we can deploy it to dreamKIT.\nStep 3: Test sdv app on HW PoC dreamKIT Same as deploy to VM, back to the tab code, click on Deploy button. Then pick dreamKIT icon and pick the dreamKIT you have from the list. Finally, click deploy. The app will be sent to dreamKIT.\nFrom dreamKIT IVI, run the prototype you have and check that the headlight blinking as expect or not.\n"
},
{
	"uri": "/engaged/create_custom_widget/",
	"title": "Create custom widget",
	"tags": [],
	"description": "",
	"content": "1. Widget introduction Widget is an element of a graphical user interface that displays information or provides a specific way for a user to interact with an application. In the context of digital.auto playground, a widget is associated to the behavior represented by one or more Vehicle APIs.\nEach widget is an isolated website, embedded into the playground dashboard by iframe. The playground supports a mechanism to set/get APIs value from/to widget.\ndigital.auto playground supports some universal built in widgets for quick experimentation. In case of advanced users who want to have more customization, e.g. more colorful, neat design, etc\u0026hellip;. This is a guideline for you to do so.\n2. Create a widget with Web Studio To create a custom widget follow bellow step.\nStep 1: in a prototype, open dashboard config.\nStep 2: Click \u0026ldquo;Pick a widget from gallery\u0026rdquo; button. Then select an \u0026ldquo;Embedded Widget\u0026rdquo;\nAfter click on Create Embedded Widget with our Web Studio wait for few seconds, Web Studio opened in new tab, and you can write your widget code there.\nOn the Web Studio tab, you can find there are some sample code already generated.\nStep 3: Remove sample code and copy and paste below code\n\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://bestudio.digitalauto.tech/project/BzR91b49OHqj/syncer.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body style=\u0026#34;color: black;font-size:12px text-align:center;padding: 8px;\u0026#34;\u0026gt; Hello! I am custom widget. \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Step 4: Change to split view mode, and click save to force the preview rerender. Then you view see the result on the right side\nStep 5: Go back the Prototype Chrome tab, click on Dashboard to check the result.\nFrom now on, you can edit html the code on studio =\u0026gt; Save =\u0026gt; double check the result with the preview panel on the right side. \u0026raquo; Finally when you happy with the look and feel. Go back to playground, reload the page to check it in playground.\n3. Access API value from widget Up to this step, we know what is a widget, and able to create a widget for the playground. Now we shall make an association between widget an a vehicle API.\nLet\u0026rsquo;s create a simple prototype that use a vehicle API. In playground Code tab, append bellow code to blink the headlight every second during a period of 10 seconds.\nfrom sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() # write your code here for i in range(10): await vehicle.Body.Lights.IsLowBeamOn.set(True) await aio.sleep(1) await vehicle.Body.Lights.IsLowBeamOn.set(False) await aio.sleep(1) Now let\u0026rsquo;s display the state of the headlight on our newly created widget. This is the most crucial step to enable the association between widget and Vehicle API.\nIn body of widget code, replace \u0026lsquo;Hello! I am custom widget.\u0026rsquo; with this code. Remember saving the code before moving on.\n\u0026lt;body style=\u0026#34;color: black;font-size:12px text-align:center;padding: 8px;\u0026#34;\u0026gt; \u0026lt;div vss=\u0026#34;true\u0026#34; vss-name=\u0026#39;Vehicle.Body.Lights.IsLowBeamOn\u0026#39;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; Next, see the result. Open the Dashboard tab, reload it, and click Run button. The outcome should look like this.\nNow, it\u0026rsquo;s free for you to add / adjust the style to match with the desired looks and feel. Here is a sample code to make the text bigger, with teal color.\n\u0026lt;body style=\u0026#34;color: black;font-size:12px text-align:center;padding: 8px;\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;font-size:24px;color:gray;text-align:center;\u0026#34;\u0026gt;Headlight Low Beam Status\u0026lt;/div\u0026gt; \u0026lt;div vss=\u0026#34;true\u0026#34; vss-name=\u0026#39;Vehicle.Body.Lights.IsLowBeamOn\u0026#39; style=\u0026#34;font-size:50px;color:teal;text-align:center;font-weight:700;\u0026#34;\u0026gt; 0 \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; And here is the result:\nNext, add two buttons to turn the headlight On and Off:\n\u0026lt;body style=\u0026#34;color: black;font-size:12px text-align:center;padding: 8px;\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;font-size:24px;color:gray;text-align:center;\u0026#34;\u0026gt;Headlight Low Beam Status\u0026lt;/div\u0026gt; \u0026lt;div vss=\u0026#34;true\u0026#34; vss-name=\u0026#39;Vehicle.Body.Lights.IsLowBeamOn\u0026#39; style=\u0026#34;font-size:50px;color:teal;text-align:center;font-weight:700;\u0026#34;\u0026gt; 0 \u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#39;text-align:center;margin-top:4px;\u0026#39;\u0026gt; \u0026lt;button vss=\u0026#34;true\u0026#34; vss-name=\u0026#39;Vehicle.Body.Lights.IsLowBeamOn\u0026#39; action=\u0026#39;set::true\u0026#39; style=\u0026#39;padding: 8px;font-size:24px;\u0026#39;\u0026gt; Turn ON \u0026lt;/button\u0026gt; \u0026lt;button vss=\u0026#34;true\u0026#34; vss-name=\u0026#39;Vehicle.Body.Lights.IsLowBeamOn\u0026#39; action=\u0026#39;set::false\u0026#39; style=\u0026#39;padding: 8px;font-size:24px;\u0026#39;\u0026gt; Turn OFF \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; Now back to Dashboard Chrome tab, reload it. And have click the buttons to see the the button working correct or not.\n4. Enhance widget with Javascript Up to now, we only use markdown language to get/set API value. It will simple make code simple and easy to understand. On this section, we will using javascript to manipulate deeper.\nIf you already satisfy with the look and feel of the widget, you can skip this part, and moving on the Secion 5: Re-use the widget\nChange the code as below:\n\u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;script\u0026gt; let API_NAME = \u0026#34;Vehicle.Body.Lights.IsLowBeamOn\u0026#34; let interval = null let textValue = document.getElementById(\u0026#34;label_value\u0026#34;) function onWidgetLoaded(options) { console.log(\u0026#34;On my widget loaded\u0026#34;) interval = setInterval(() =\u0026gt; { if(textValue) { let apiValue = getApiValue(API_NAME) console.log(\u0026#34;type of apiValue\u0026#34;, typeof apiValue) textValue.innerText = apiValue } }, 500) } function onWidgetUnloaded() { console.log(\u0026#34;On my widget unloaded\u0026#34;) if(interval) clearInterval(interval) } let btnSetOn = document.getElementById(\u0026#34;btnSetOn\u0026#34;) let btnSetOff = document.getElementById(\u0026#34;btnSetOff\u0026#34;) btnSetOn.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { setApiValue(API_NAME, true) }) btnSetOff.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { setApiValue(API_NAME, false) }) \u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://bestudio.digitalauto.tech/project/BzR91b49OHqj/syncer.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body style=\u0026#34;color: black;font-size:12px text-align:center;padding: 8px;\u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;font-size:24px;color:gray;text-align:center;\u0026#34;\u0026gt;Headlight Low Beam Status\u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#39;label_value\u0026#39; style=\u0026#34;font-size:50px;color:teal;text-align:center;font-weight:700;\u0026#34;\u0026gt; 0 \u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#39;text-align:center;margin-top:4px;\u0026#39;\u0026gt; \u0026lt;button id=\u0026#34;btnSetOn\u0026#34; style=\u0026#39;padding: 8px;font-size:24px;\u0026#39;\u0026gt; Turn ON \u0026lt;/button\u0026gt; \u0026lt;button id=\u0026#34;btnSetOff\u0026#34; style=\u0026#39;padding: 8px;font-size:24px;\u0026#39;\u0026gt; Turn OFF \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; You will get the same result but with get/set by javascript function.\nLet explain deeper: First, we need attach the library:\n\u0026lt;script defer src=\u0026#34;https://bestudio.digitalauto.tech/project/BzR91b49OHqj/syncer.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; The library support bellow functions:\n// this function be called when your widget mounted function onWidgetLoaded(options) { console.log(\u0026#34;On my widget loaded\u0026#34;) } // this function be called when your widget unmounted function onWidgetUnloaded() { console.log(\u0026#34;On my widget unloaded\u0026#34;) if(interval) clearInterval(interval) } And you also have two API to get and set API.\nlet apiValue = getApiValue(API_NAME) setApiValue(API_NAME, value) 5. Re-use the widget On digital.auto -\u0026gt; Widget config, you can see an \u0026ldquo;open\u0026rdquo; button to launch Web Studio anytime you want to edit it.\nThe link in widget -\u0026gt; options -\u0026gt; url: you can copy an it and reuse in other prototype or share it to your co-worker.\nYou can also can right click on index.html file to copy the link. It\u0026rsquo;s the same link as above.\n"
},
{
	"uri": "/basics/login/",
	"title": "Login",
	"tags": [],
	"description": "",
	"content": "Before you begin exploring our rapid prototyping environment for Vehicle Apps, let\u0026rsquo;s ensure you\u0026rsquo;re familiar with our login process. Follow this step-by-step guide for a smooth start:\nClick \u0026ldquo;Login\u0026rdquo; in the top right conner of the playground homepage to open the login popup.\n1. Login From here, you can:\nLog In: If you already have an account, click on the \u0026ldquo;Login\u0026rdquo; button, and follow below procedure.\nSign Up: New around here? No worries. Then refer to the \u0026ldquo;Sign-up\u0026rdquo; section below for a guided walk-through.\nForgot Password?: Can\u0026rsquo;t remember your password? Then process to the \u0026ldquo;Forgot Password\u0026rdquo; section for assistance.\nSteps: Email: Input your registered email. Password: Enter your account password. Click the \u0026ldquo;Login\u0026rdquo; button. 💡 Quick Tip: You can swiftly log in with your GitHub credentials. Click on the \u0026ldquo;Continue with GitHub\u0026rdquo; option for immediate access.\n2. Sign Up Are you new here? Let\u0026rsquo;s get you set up.\nIf you don\u0026rsquo;t have an account yet, simply click \u0026ldquo;Don\u0026rsquo;t have an account? Sign Up\u0026rdquo; from the Login popup.\nSteps: Name: Provide your full name. Email: Use a valid email address. Password: Choose a strong password. Confirm Password: Re-enter your password for accuracy. CAPTCHA: Please complete the CAPTCHA to verify. Click the \u0026ldquo;Sign Up\u0026rdquo; button to create your account. 3. Forgot Password Forgot your credentials? We\u0026rsquo;ve got your back.\nSelect \u0026ldquo;Forgot Password\u0026rdquo; to reset your password.\nSteps: Retrieve Email: Input the email you registered with. CAPTCHA: Complete the CAPTCHA check. Click the \u0026ldquo;Confirm\u0026rdquo; button, and we\u0026rsquo;ll send you reset instructions. If you recall your password while here, simply choose \u0026ldquo;Back to Login.\u0026rdquo;\n"
},
{
	"uri": "/dreamkit/working/ecuplugplay/ecu-how-to-identify-your-ecu/",
	"title": "Identify your ECU with dreamKIT",
	"tags": [],
	"description": "",
	"content": "In this section, we will help you to identify the requirements to successfully connect an ECU to dreamKIT. The necessary information will consists of the dreamKIT hardware specifications and the communication identifiers on your ECU.\nWhat you will achieve By following this guide, you can register any ECU to the dreamKIT. You will be confident to connect any ECU for your own development purpose.\n1. dreamKIT hardware specifications Currently, our dreamKIT supports Controller Area Network (CAN) bus, both Standard CAN and CAN-FD. Please ensure your ECU has CAN protocol.\n1.1. Explore the CAN communication ports The CAN communication on your dreamKIT is divided into two groups of ports: CAN0 group and CAN1 group. Below, you will find an illustration showing the exact location of these ports: 1.2. Default CAN configurations Below are the default settings of the CAN buses on dreamKIT. This defines how the CAN ports operate out of the box. Starting a project with the default configurations will minimize the setup process:\nCAN0 CAN1 Type CAN FD Standard CAN Data rate 500/2000 kbps 500 kbps Data length up to 64 bytes up to 8 bytes 1.3. Anatomy of a CAN port Each CAN port consists of 6 pins:\nPin Function 1 CAN LOW (CANL) 2 CAN HIGH (CANH) 3 VCC 6 GND Each connector is created not only to provide a stable 12V power supply but also to support CAN communication between the dreamKIT and connected ECUs. This dual functionality simplifies the connections between the hardware during our development process.\n2. Configure the CAN settings While the default configuration of the dreamKIT’s CAN ports is designed to meet a wide range of needs, specific projects may require custom settings. If the default settings do not align perfectly with ECU\u0026rsquo;s requirements, you have the flexibility to modify these settings to better suit your project’s demands.\nThe configuration options for the CAN settings are accessible through the S32G Gateway. To customize these settings, you’ll need to establish a secure remote connection. Here’s how you can get started:\nEstablish an SSH Connection: First, an RJ45 Ethernet cable is used to link the dreamKIT to our PC: Have a look on the dreamKIT\u0026rsquo;s IVI, we can observe the pre-defined Ethernet IP provided by our dreamKIT: For our PC to recognize this Ethernet connection, we need to reconfigure the IP address on the Ethernet port of our PC to have the same network 192.168.56.xx/24. We can do it by following the steps on our Windows PC: Control Panel -\u0026gt; Network and Internet -\u0026gt; Network and Sharing Center, then follow up with the steps below: By ensuring both your PC and the dreamKIT are configured to the same subnet of 192.168.56.00/24, data exchange and connectivity is allowed across the network.\nSetting up an SSH connection to the Gateway: This secure method ensures that your adjustments are safe and private. The command format is: $ ssh [username]@[host_ip_address].\nAccess Credentials: To obtain the necessary username and host_IP_address for the Gateway, please have a look at the table below. The information will be used again and again during your work with dreamKIT, so please note it somewhere easy to access.\nJetson Orin VCU S32G Gateway username sdv-orin bluebox host_IP_address 192.168.56.48 192.168.56.49 password 123456 123456 Note: If you lack any information while following this guide, please feel free to contact our support team. They are ready to assist you with the information you need to access your system.\nTo SSH to your Gateway:\n$ ssh bluebox@192.168.56.49 Our default CAN protocols will have the properties like this: Make some changes to the CAN settings: To apply changes to a CAN interface, first disable it with: $ sudo ip link set [your_can_interface] down Configure the CAN interface: For a standard CAN interface: $ sudo ip link set [your_can_interface] type can bitrate [your_desire_bitrate]. For a CAN-FD interface: $ sudo ip link set [your_can_interface] type can bitrate [your_desire_bitrate] sample-point [sample_rate] dbitrate [CAN_bit_rate] fd on. Lastly, you will need to re-enable the CAN interface: $ sudo ip link set [your_can_interface] up. Now your settings should be applied successfully. Example configuration Suppose you want to switch your CAN1 interface from standard CAN to CAN-FD with a bitrate of 100000. After disabling the CAN1 interface, you would use the command:\n$ sudo ip link set can1 type can bitrate 100000 sample-point 0.75 dbitrate 2000000 fd on Then, reactivate it and check the modifications: Now, your CAN1 interface should show the maximum transmission unit (MTU) of 72, confirming that it is operating under the CAN-FD protocol!\n3. Register your ECU to dreamKIT 3.1. ECU information fields in EcuList.json The detailed registration information for each ECU connected to the dreamKIT is meticulously stored in the EcuList.json file, which is located in /opt/data/ by default on both the VCU and the Gateway of your dreamKIT system.\nThis critical file contains all the registered ECU information, enabling the dreamKIT to accurately recognize and identify each connected ECU along with its specific properties. To effectively integrate an ECU with your system, you must provide the following key information fields in the EcuList.json:\nField name Description \u0026ldquo;name\u0026rdquo; This is the name of your ECU \u0026ldquo;comType\u0026rdquo; Type of communication \u0026ldquo;channel\u0026rdquo; Channel of the communication type \u0026ldquo;reqToEcuID\u0026rdquo; Request to ECU ID \u0026ldquo;resFromEcuID\u0026rdquo; Response from ECU ID \u0026ldquo;triggerMsgID\u0026rdquo; Trigger Message ID \u0026ldquo;pathToSecCheck\u0026rdquo; Path to the security file \u0026ldquo;DID_Info\u0026rdquo; Data Identifier (DID) of the UDS protocol on ECU Your EcuList.json file should looks like this:\nOne essential parameter to focus on is the \u0026ldquo;triggerMsgID\u0026rdquo;. This ID is periodically sent by the ECU and serves as a unique identifier for the dreamKIT to acknowledge the presence and identity of the connected ECU.\nIn our specific scenario, when the dreamKIT receives a CAN message with the ID value of 0x500, it identifies the connected ECU as the dreamPACK_ECU. This ID acts as a signature, allowing the dreamKIT to distinguish the dreamPACK_ECU from other devices and ensuring that all interactions and communications are correctly attributed and processed.\nBy setting and monitoring the \u0026ldquo;triggerMsgID\u0026rdquo;, the dreamKIT can effectively manage multiple ECUs, each potentially with its unique identifier, thus enhancing the system\u0026rsquo;s ability to scale and accommodate diverse automotive applications. 3.2. Updating the EcuList.json file in dreamKIT Once you have filled in all necessary details for your ECU in the EcuList.json, ensure that this file is updated on both the VCU and the Gateway to maintain system consistency.\nTo facilitate file transfers between the VCU and the Gateway, we use the scp command. This command allows for the secure copying of files across the network:\n$ scp -r \u0026#39;/source_path/source_file_folder\u0026#39; \u0026#39;/destination_path/destination_file_folder\u0026#39; After filling your ECU information into the EcuList.json file, you need to synchronize and update the EcuList.json on both the VCU and the Gateway. Suppose that you are adding your ECU to the list in the Gateway, then you will need to copy that EcuList.json from the Gateway to the VCU by using this command line:\n$ scp -r \u0026#39;/opt/data/EcuList.json\u0026#39; \u0026#39;sdv-orin@192.168.56.48:/opt/data/\u0026#39; 3.3. Restart dreamKIT to apply changes After updating your ECU information and synchronizing the EcuList.json file between the Gateway and the VCU, you need to restart the dreamKIT so that changes can be applied to our system. You just need to toggle the physical power switch on the dreamKIT. The ECU that you have configured should then be recognized automatically.\n4. Plug your ECU into the dreamKIT Upon successfully powering up your dreamKIT, the In-Vehicle Infotainment (IVI) will display as follows: Each ECU connects to the dreamKIT using one of the combined power and CAN ports of your choice: Internally, the dreamKIT’s software immediately begins checking for the periodically sent \u0026ldquo;triggerMsgID\u0026rdquo; sent from any connected ECUs. This ID helps the system recognize whether the connected ECU has been predefined in the system’s configuration.\nIf the ID matches a predefined one, the IVI will display a notification: \u0026ldquo;An ECU is just connected to your system\u0026rdquo;: Under the \u0026ldquo;Service\u0026rdquo; tab on the IVI, you can view both registered and currently connected ECUs: When a registered ECU, like the dreamPACK, connects and communicates using specific CAN messages (e.g., ID 0x500), the dreamKIT identifies and displays the ECU’s details on the IVI under the \u0026ldquo;Service\u0026rdquo; tab.\nNeed assistance? Contact our development team Should you have further interest in dreamKIT, please send us query here https://forms.office.com/e/BiJcMEhbBy\nOur team is committed to providing you with the necessary support to address your queries and challenges promptly.\n"
},
{
	"uri": "/dreamkit/working/ecuplugplay/",
	"title": "ECU Plug and Play",
	"tags": [],
	"description": "",
	"content": "This guide aims to help you Plug your own ECU to dreamKIT, and Play your software on your ECU.\n"
},
{
	"uri": "/dreamkit/retrofit_kit/",
	"title": "Co-Creation Challenge Vehicle KIT",
	"tags": [],
	"description": "",
	"content": "Hardware Basics 1. Nvidia Jetson Nano CPU: Quad-core ARM A57 @ 1.43 GHz GPU: 128-core Maxwell Memory: 4 GB 64-bit LPDDR4 URL: https://developer.nvidia.com/embedded/jetson-nano-developer-kit\n2. Uninterruptible Power UPS for Jetson Nano with 4x 3500mAh 18650 Akku\n3. WiFi Module Wi-Fi 5 and Bluetooth 4.2 Module 2.4G/5GHz dual-band WiFi URL: https://www.waveshare.com/wireless-ac8265.htm\n4. GPS and 4G Communication Module 2G/3G/4G network connection GNSS connector, supports GPS, Beidou, Glonass, LBS base station positioning URL: https://www.waveshare.com/sim7600g-h-4g-dongle.htm\n5. Storage Size: 128 GB Speed: 130MB/s A2- und V30-Classified\n6. Camera 1 (Front seats) \u0026amp; 2 (Rear seats), Optional 3 (Windshield) (1,2 Installed) Night Vision USB Camera with IR LEDs 170° fisheye lens 1920 x 1080 at 30 fps; 1280 x 720 at 60 fps; 640 x 480 at 100 fps\n7. Microphone (Installed) USB-Connected 360° omnidirectional audio Mute button Notice-Cancelling\n8. Bluetooth OBD II Scanner ELM 327\n9. Display (Installed) 7\u0026quot; HDMI LCD 1024*600 IPS Touchscreen\nSensor Kit (Installed) 1. mmWave Radar Sensor 60- to 64-GHz 4 RX, 3 TX with 120° azimuth FoV and 30° elevation FoV URL: https://www.ti.com/tool/IWR6843ISK\n2. Nicla Sense ME (Installed) 16-bit 6-axis IMU 24-bit absolute barometric pressure sensor 12-bit 3-axis geomagnetic sensor Gas sensor with integrated pressure, humidity, and temperature URL: https://docs.arduino.cc/hardware/nicla-sense-me\n3. Infrared CO2 Sensor Vin: Power Positive (Vin) GND: Negative Power Supply (GND) Vo: analogue output. PWM: PWM. HD: HD (zero for school, low for more than 7 seconds) Rx: TTL level data input for UART (RXD) Tx: TTL data output with UART (TXD) level. Pin 1: Analog Output Vo\n4. Laser PM2.5 Sensor Features: Interface specification: 1: CTL: control pin, reserved 2: 1um:\u0026gt; 0.3 micron particle concentration, PWM output. 3: 5 V: 5 V power input. 4: 25um:\u0026gt; 2.5 micron particle concentration, PWM output 5: GND: GND 6: R: Receiver for serial port RX 7:T: Transmission via serial port TXPS: the distance of the interface is 2.54mm\nCommunication protocol: Serial communication protocol: 9600 8N1 (rate of 9600, data bits 8, parity none, stop bits 1) Communication cycle for series reports: 1 + 0.5 seconds. Data frame (10 bytes): message header + job + data (6 bytes) + checksum + message trailer. The number of bytes: Name: Content 0:Message Head:AA 1: order: c0 2: Data 1: PM2.5 low-byte. 3: Data 2: PM2.5 high byte 4: Data 3: PM10 low-byte. 5: Data 4: PM10 high byte 6: data 5: 0 (reserved) 7: Data 6: 0 (reserved) 8: Checksum: Checksum 9:Message Trailer:AB\nChecksum: Data 1 + Data 2 + \u0026hellip; + Data 6 PM2.5 data content: PM2.5 (ug / m3) = ((PM2.5 high-byte x 256) + PM2.5 low-byte) / 10. PM10 data content: PM10 (ug / m3) = ((PM10 high-byte x 256) + PM10 low-byte) / 10.\nActuators 1. Backseat fan (Installed) 2. Portable Espresso machine (Installed) 3. LED text display for back window (Installed) 4. Car air purifier (Installed) 5. Massage cushion (Installed) 6. Ambient lights 7. Head-up display Software OS Operating System: Ubuntu 20.04.\nKuksa Kuksa Databroker is available on localhost: 55555 providing the available Vehicle and Sensor Information as VSS Values.\nVelocitas Velocitas Python SDK is installed on the Jetson Nano. SDV applications can be created using Python templates following the online tutorials: https://eclipse.dev/velocitas/docs/tutorials/vehicle_app_development/python_development/\nExample Discomfort recognition use case with Velocitas and Landing AI This documentation provides a step-by-step guide on creating a Discomfort Recognition application using Velocitas Python template and the LandingAI API on a Jetson Nano system.\nStep 1. Folder Setup a. Create a new folder in the Jetson Nano system at the following path:\n/usr/bin/dreamkit/prototypes Use a unique identifier as the folder name.\nb. Inside the created folder, add a new Python file named main.py.\nStep 2. Edit prototypes.json a. Locate and edit the prototypes.json file in the following path:\n/usr/bin/dreamkit/prototypes b. Add a new entry for the Discomfort Recognition use case:\n{ \u0026#34;id\u0026#34;: \u0026#34;YOUR_UNIQUE_IDENTIFIER\u0026#34;, \u0026#34;lastdeploy\u0026#34;: \u0026#34;CURRENT_DATE_TIME\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Discomfort Recognition\u0026#34; } Replace YOUR_UNIQUE_IDENTIFIER with the same name as the folder created in Step 1 and update CURRENT_DATE_TIME with the current date and time.\nStep 3. Python Code import cv2 import logging as log import datetime as dt from time import sleep from PIL import Image from landingai.predict import Predictor import json # Path to the Haar Cascade file for face detection cascPath = \u0026#34;haarcascade_frontalface_default.xml\u0026#34; faceCascade = cv2.CascadeClassifier(cascPath) # Configure logging log.basicConfig(filename=\u0026#39;webcam.log\u0026#39;, level=log.INFO) # Initialize the video capture from the default camera (camera index 0) video_capture = cv2.VideoCapture(0) anterior = 0 font = cv2.FONT_HERSHEY_SIMPLEX label_name = \u0026#34;normal\u0026#34; # Enter your LandingAI API Key and Endpoint ID endpoint_id = \u0026#34;Endpoint_ID\u0026#34; api_key = \u0026#34;Your_API_KEY\u0026#34; while True: if not video_capture.isOpened(): print(\u0026#39;Unable to load camera.\u0026#39;) sleep(5) pass # Capture frame-by-frame ret, frame = video_capture.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Detect faces in the grayscale frame faces = faceCascade.detectMultiScale( gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30) ) # Draw rectangles around detected faces for (x, y, w, h) in faces: cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2) if anterior != len(faces): anterior = len(faces) log.info(\u0026#34;faces: \u0026#34; + str(len(faces)) + \u0026#34; at \u0026#34; + str(dt.datetime.now())) # Capture a new frame check2, frame2 = video_capture.read() # Save the current frame as an image cv2.imwrite(filename=\u0026#39;image.png\u0026#39;, img=frame2) # Load the saved image image = Image.open(\u0026#34;image.png\u0026#34;) # Run inference using LandingAI Predictor predictor = Predictor(endpoint_id, api_key=api_key) predictions = predictor.predict(image) # Extract information from predictions score = predictions[0].score if predictions[0].label_name: label_name = predictions[0].label_name # Draw a black background rectangle for displaying the label cv2.rectangle(frame, (50, 50), (225, 125), (0, 0, 0), -1) # Add the predicted label text to the frame cv2.putText(frame, label_name, (50 + int(125/10), 50 + int(75/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) # Display the resulting frame with live video and predictions cv2.imshow(\u0026#39;Video\u0026#39;, frame) # Check for key presses key = cv2.waitKey(1) if key \u0026amp; 0xFF == ord(\u0026#39;s\u0026#39;): # Save the current frame as an image check, frame = video_capture.read() cv2.imshow(\u0026#34;Capturing\u0026#34;, frame) cv2.imwrite(filename=\u0026#39;saved_img.jpg\u0026#39;, img=frame) video_capture.release() img_new = cv2.imread(\u0026#39;saved_img.jpg\u0026#39;, cv2.IMREAD_GRAYSCALE) img_new = cv2.imshow(\u0026#34;Captured Image\u0026#34;, img_new) cv2.waitKey(1650) print(\u0026#34;Image Saved\u0026#34;) print(\u0026#34;Program End\u0026#34;) cv2.destroyAllWindows() break elif key \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): # Turn off the camera and exit the program print(\u0026#34;Turning off camera.\u0026#34;) video_capture.release() print(\u0026#34;Camera off.\u0026#34;) print(\u0026#34;Program ended.\u0026#34;) cv2.destroyAllWindows() break The result of our application on Jetson Nano: Appendix List of available VSS signals with data stream mapping from the sensors. Vehicle.Acceleration.Longitudinal\rVehicle.Acceleration.Lateral\rVehicle.Acceleration.Vertical\rVehicle.AngularVelocity.Roll\rVehicle.AngularVelocity.Pitch\rVehicle.AngularVelocity.Yaw\rVehicle.Cabin.HVAC.AmbientAirTemperature "
},
{
	"uri": "/dreamkit/working/",
	"title": "Working",
	"tags": [],
	"description": "",
	"content": "Listing concrete examples on DreamKit\n"
},
{
	"uri": "/advanced/setup-instance/",
	"title": "Setup autowrx instance",
	"tags": [],
	"description": "",
	"content": "This guide will walk you through the steps to set up, run, and customize an instance of the open-source autowrx project.\nPrerequisites Before proceeding with the setup, ensure that you have the following prerequisites installed on your system:\nnode (version 20.12.12 or higher) Yarn (optional, if installed, it needs to be installed globally) Setting up autowrx Follow the steps below to set up your instance of autowrx:\nClone the Repository\nFirst, clone the autowrx repository from GitLab. You can choose to clone the repository using either HTTPS or SSH.\nTo clone with HTTPS, use the following command:\ngit clone https://gitlab.eclipse.org/eclipse/autowrx/autowrx.git To clone with SSH, use this command:\ngit clone git@gitlab.eclipse.org:eclipse/autowrx/autowrx.git Install dependencies\nNavigate into the directory of the cloned repository and install all necessary dependencies.\nUsing Yarn:\ncd autowrx yarn Using npm:\ncd autowrx npm i Add a .env File\nNext, you need to create a .env file in the root directory of the project. This file will contain configuration variables, such as the server endpoint. Below is an example of what your .env file should look like:\nVITE_SERVER_BASE_URL=https://autowrx-server.example.com Replace https://autowrx-server.example.com with the actual server endpoint of your autowrx instance.\nRun project\nFinally, you can run the project using either Yarn or npm. Choose one of the following commands:\nUsing Yarn:\nyarn dev Using npm:\nnpm run dev Your autowrx instance should now be up and running at localhost:3000, ready for you to customize and use.\nCustomize your instance After setting up your instance, you can customize its appearance and content. The files and configurations you need to edit for customization are listed and described below.\nEditable files for customization The image below contains the files you can edit: The following files, starting from the root of your project directory, can be customized:\npublic/imgs/favicon.ico: This file is the small favicon that appears on the browser tab. public/imgs/logo-wide.png: This file is the logo of your instance. src/configs/config.ts: This file contains various configuration settings. src/index.css: This file contains the styling for your instance. instance.ts: This file allows you to update some content of the instance. Note: If any of the above files do not exist in your project, please create a new file with the exact name.\nFor more detailed information about the config.ts, index.css, and instance.ts files, refer to the instructions below.\nsrc/configs/config.ts Refer to the table below for the meaning of properties.\nName Description serverBaseUrl Base URL of API serverVersion Version of API instanceLogo Logo of your instance genAI Related configurations for GenAI feature genAI.defaultEndpointUrl Default endpoint for calling GenAI genAI.marketplaceUrl The endpoint for listing GenAIs. Default is https://store-be.digitalauto.tech/marketplace/genai genAI.sdvApp Array of objects. Each object contains custom GenAI beside listed GenAIs from genAI.marketplaceUrl genAI.sdvApp.id Id of the GenAI app genAI.sdvApp.type Type of GenAI. Either GenAI_Widget, GenAI_Python or GenAI_Dashboard genAI.sdvApp.description Description of GenAI app genAI.sdvApp.endpointUrl Endpoint URL for calling GenAI. If left blank, it will use genAI.defaultEndpointUrl as a fallback genAI.sdvApp.customPayload Customize the payload (body) when calling the GenAI src/index.css Scroll down to the bottom of the file (at Font and Colors for instances section). Here are all the properties you can edit to update appearance of the instance globally. Note: All the color values in the image above are in HSL format. You can update the value with different formats (eg. RGBA, HEX).\nRefer to the table below for the meaning of variables:\nVariable Description \u0026ndash;da-primary-100 Primarily used as the background color for various elements. \u0026ndash;da-primary-300 A median color between \u0026ndash;da-primary-100 and \u0026ndash;da-primary-500. \u0026ndash;da-primary-500 Mainly used in primary text and certain borders. \u0026ndash;da-secondary-100 Light secondary color. \u0026ndash;da-secondary-300 Medium secondary color. \u0026ndash;da-secondary-500 Secondary color, often used for less important text. \u0026ndash;da-accent-100 Light accent color. \u0026ndash;da-accent-300 Medium accent color. \u0026ndash;da-accent-500 Accent color, used in various text elements. \u0026ndash;da-white The white color, mainly used for text and background elements. \u0026ndash;da-black The black color, used for some text and background elements. \u0026ndash;da-gray-light Light gray color, primarily used for text, background, and borders. titlece. \u0026ndash;da-gradient-from The starting color of a gradient, used for primary buttons and background image overlay on the homepage. \u0026ndash;da-gradient-to The ending color of a gradient, used for primary buttons and background image overlay on the homepage. instance.ts Refer to the table below for the meaning of properties.\nName Description name Name of the instance policy_url URL to your policy partners List of your partners. This will be displayed at the bottom of homepage partners.category Name of the group (category) of partners partners.items List of partners in a group partners.items.name Name of the partner partners.items.img Logo of the partner partners.items.url URL to partner\u0026rsquo;s website text Some text content in your instance text.home_ads_pan_title The title of the hero banner on the homepage text.home_ads_pan_desc The description of the hero banner on the homepage "
},
{
	"uri": "/basics/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "What is playground.digital.auto? The playground.digital.auto is a cloud-based, rapid prototyping environment for new, SDV-enabled features. The prototypes are built against real-world vehicle APIs and can be seamlessly migrated to automotive runtimes, such as Eclipse Velocitas. The playground is open and free to use at playground.digital.auto.\nThe playground is designed to foster continuous, customer-centric digital innovation. By supporting the new way of thinking and working digital first, digital.auto enables developers to deliver fast tangible results and early validation. Prototypes are developed in Python. To interact with vehicle sensors and actuators, the COVESA Vehicle Signal Specification (VSS) is used. In the browser environment of the playground, the vehicle sensors and actuators are mocked, using simple test values. Access to VSS in Python is provided via the emerging VSS Python mapping, as defined by the Eclipse Velocitas project (part of Eclipse SdV). As we will discuss in the following, access to more sophisticated vehicle simulation environments or even real sensors and actuators is possible via a cloud bridge mechanism.\nAlso, please check out the following additional resources:\nOverview video of digital.auto playground Introduction to digital.auto playground plugin development digital.auto playground widget documentation Why playground.digital.auto? The playground.digital.auto enables early prototyping which has two main benefits:\nCustomer-centric innovation from the start: Early customer feedback helps to learn which ideas have highest potential for customer value creation. This helps minimizing investments in unpopular features. Fine-tuning the customer journey design early is key for customer acceptance. Of course this does not mean that the software should not constantly be improved later on, even after the start of production. After all, this is why DevOps pipelines with OTA for remote vehicle updates are currently being established.\nFast delivery of tangible, validated results: Doing early prototyping also has many benefits from the development perspective: Having a functional mockup early on in the development cycle helps improving transparency between business/IT, across regional and organizational boundaries. It also helps to validate architecture decisions early on, as well as to have a consistent enterprise architecture across all features. Finally, being able to identify API requirements as early as possible is key, because providing an API which encapsulates hardware usually has a very long lead time. This is especially true for hardware and APIs coming from external suppliers.\nValue streams The following is looking at both, the digital and physical value stream in digital.auto, followed by a discussion of the evolution of code in the digital value stream.\nDigital and physical value stream The digital.auto playground is designed to support the general philosophy of digital.auto, which is assuming two distinct value streams, moving at different speeds: The physical and the digital value stream. These two value streams are de-coupled via a Hardware Abstraction Layer (HAL), which is encapsulating the complexities of vehicle physics, embedded systems, and bus systems. Software components developed in the digital value stream are accessing vehicle functions via well defined interfaces (e.g. VSS). This de-coupling allows components north and south of the API to be (more or less) seamlessly interchanged. For example, a prototype in the playground can first run against simple test values provided by VSS mock implementations in the playground. Next, one might plug in a real vehicle simulation, running south of the API, and providing a more realistic system behavior. Finally, the simulation will be replaced by hardware with real sensors and actuators - starting with a breadboard, and eventually the final vehicle.\nSimilarly, north of the API, new SdV features can initially be developed in Python in the playground. Next, the SdV prototype code can be deployed to a professional development environment - as provided, for example, by Eclipse Velocitas. In this environment, the new SdV feature can first be tested in the cloud, before finally being deployed on a vehicle computer.\nCode evolution in the digital value stream In order for the SdV code not to break when moving from the playground to the real development environment, the VSS Python APIs are currently being standardized by the Eclipse community. This allows to migrate code more easily between different environments. For example, as described in the figure below, an SdV function might initially be implemented as a prototype in the digital.auto playground. After the initial customer validation, the decision is made to migrate the code from the prototyping environment to the professional development environment, including proper support for CI/CD. This can be done easily because of the standardization of the Python APIs. In fact, the next release of the playground will have built-in support to deploy into Eclipse Velocitas by creating a complete Velocitas project in GitHub, based on the initial prototype.\nOne important note: Even if the final target language for the production system is not Python - but maybe C++ or Rust - having a Python prototype for early vehicle tests is extremely valuable, because it helps getting an end-to-end implementation done quickly, and stabilizing the APIs between the distributed components.\nVSS Python API The VSS API is organized in a strict tree hierarchy. The nodes of the VSS tree are called branches. The leaves in the tree are representing sensors, actuators, and attributes. An example for a sensor representation in VSS looks like this:\nVehicle.Cabin.Seat.Row1.Pos1.IsBelted This VSS API or data point will return a boolean value, indicating whether the belt is engaged. Please note that the VSS API catalogue can be adapted for individual vehicle instances - for example, supporting vehicles with different numbers of seat rows.\nAn example for an actuator in VSS is shown in the following:\nVehicle.Cabin.Seat.Row1.Pos1.BackrestRecline This API will control the seat z-axis depends on seat tilt.\nget()/set() functions Using the VSS API in Python is straight forward. For example, to get the current state of a seat belt, the following code can be used:\nfrom ACME_Car_EV_v01 import Vehicle\rvehicle = Vehicle()\rvehicle.Cabin.Seat.Row1.Pos1.IsBelted.get() The get() function will simply return the current state of the IsBelted sensor represented by the corresponding Python object in the digital.auto playground library.\nNot very surprisingly, to control an actuator, a set() API is provided in Pyhton, e.g.:\nvehicle.Cabin.Seat.Row1.Pos1.BackrestRecline.set(25) However, as straight-forward as this code actually looks like, the logic behind it is not as straight forward - the reason being that this API is supposed to control a physical device, which might not react immediately to the request (or maybe not at all). From the VSS specification: “Actuators are used to control the desired value of a property. Some properties in a vehicle cannot change instantly. A typical example is position of a seat or a window. Reading a value of an actuator shall return the current actual value, e.g. the current position of the seat, rather than the wanted/desired position. A typical example could be if someone wants to change the position of a seat from 0 to 100. This can be changed by setting the corresponding actuator to 100. If the actuator is read directly after the set request it will still return 0 as it might take some seconds before the seat reaches the wanted position of 100. If the seat by some reason is blocked or cannot be moved due to safety reasons it might never reach the wanted position. It is up to the vehicle to decide how long time it shall try to reach the desired value and what to do if it needs to give up.”\nEven though BackrestRecline is encapsulating an actuator, its current value can be read using get():\nvehicle.Cabin.Seat.Row1.Pos1.BackrestRecline.get() However, as per the above discussing please note that it might not be 100% clear what the return value is actually indicating. Note that there is work going on at the moment to support APIs which differentiate between the actual value vs the intended value of an actuator.\nFinally, please also note that the current version of VSS used here is not supporting meta data in the API that could be used to support additional service level, including real-time requirements, request priorization, etc. This means that the SdV code using VSS currently is aiming at applications which are labeled as \u0026ldquo;QM\u0026rdquo;, according to the ASIL standard - meaning the code does not support any of the higher ASIL safety levels, such as ASIL A, B, C or D.\nEvent listeners When dealing with sensor data - even for mocked sensors - it can often make sense to use a more event-driven model, instead of constantly polling the sensor. To support this, the Python API for VSS is supporting a simple event-driven programming model. Using the subscribe() method, a callback function can be associated with a sensor. This function will be called every time a new value is available.\nIn the following code sample, a new function on_hood_is_open_changed is defined, and then associated with vehicle.Body.Hood.IsOpen via the subscribe() method. After this, a mock wiper is turned on to MEDIUM speed. Next, the mock hood is opened. This will result in on_hood_is_open_changed being called, which in turn will turn the wipers off.\ndef on_hood_is_open_changed(IsOpen: bool):\rif IsOpen:\rvehicle.Body.Windshield.Front.Wiping.Mode.set(vehicle.Body.Windshield.Front.Wiping.Mode.OFF)\rvehicle.Body.Hood.IsOpen.subscribe(on_hood_is_open_changed)\rvehicle.Body.Windshield.Front.Wiping.Mode.set(vehicle.Body.Windshield.Front.Wiping.Mode.MEDIUM)\rvehicle.Body.Hood.IsOpen.set(True) Playground architecture The following provides an overview of the playground architecture, as well as the key elements of the plugin concept.\nThe digital.auto playground is designed to allow execution of SdV Python code against the standard VSS Python API. The SdV Python code is executed in the browser, against a set of Python objects representing the VSS API. To ensure a good user experience, the playground also has to support easy manipulation of the HTML Document Object Model (DOM), as well as remote interaction with the cloud. Since most browser development is done in JavaScript these days, the playground supports a plug-in mechanism which is implemented in JavaScript. This means that the SdV Prototypes in Pyhton are really interacting with VSS objects which are implemented in JavaScript. This way, a vehicle mockup can easily be built using browser-native tools. The mechanism used here is a Python-to-JavaScript bridge, which is translating between the SdV functions in Python and the plug-ins in JavaScript.\nThe playground is trying to maximize re-use. This is happening on two levels:\nWidgets: Re-useable Java Script artefacts for visualization of any kind of sensor Plug-Ins: Sensor-specific UI elements, i.e. they are mapping specific sensor values to generic widgets The figure below describes how to get from widget to plug-in and eventually to the creation of custom dashboards using specific plug-ins.\nPlayground plugins Plugins for the digital.auto playground are usually providing a mockup and a visualization of a specific vehicle feature, exposed via VSS. An example could be a Google maps plugin, which is visualizing the vehicle\u0026rsquo;s current position by accessing Vehicle.CurrentLocation.\nA plugin can provide two things:\nWidgets which are providing the UX to visualize the behavior represented by one or more VSS APIs Simulators which are implementing VSS APIs An SdV prototype that wants to make use of a plugin must do two things:\nImport the plugin Python library Configure the plugins used (especially the UX layout) Plugins can be combined in different ways. For example, an SdV prototype might import the Google maps plugin to visualize the vehicle`s position on a map, plus another plugin which is actually implementing Vehicle.CurrentLocation. Or, the UX and the VSS implementation might be combined in one plugin, depending on the design. The digital.auto project is aiming to build up a rich plug-in library over time. However, often it will be required to implement dedicated plugins for a specific prototype. To support this, the plugin mechanism in the playground is open, both for plugin consumption as well as provisioning.\nFor the implementer of a plugin, the following provides more detailed instructions for doing so.\nSimulators By default, the VSS Python API is providing an implementation which provides a very basic functionality: the get() functions are returning the current value (initialized randomly). The set() functions are storing the value passed to them.\nSimulators are providing a way to change this functionality. They allow plug-in developers to implement more specific functions for a given VSS sensors, actuator, or attribute. For example, a get function could communicate with a remote service in the cloud to receive the current sensor value. This can be any cloud, assuming that Cross-Origin Resource Sharing (CORS) or an alternative mechanism is used.\nA simulator can also be combined with a widget, e.g. to visualize a new sensor value.\nWidgets Widgets can use the built-in grid area of the playground to visualize vehicle functions, or even provide an interactive experience - using standard browser functionality. The grid is divided into 5x2 grid cells. A widget can occupy 1 or more grid cells, always assuming a rectangular shape.\nThe grid mechanism is designed as simple as possible (we wanted to avoid the complexities of a full-blown portal server), yet giving a lot of flexibility. Each widget is mapped to an iFrame, meaning that a widget can use all functionalities which an iFrame supports.\nWhen designing a widget, using the right size must be ensured. For example, the ideal size for a 2x1 box video in the grid would be: 1080x756\nIt doesn\u0026rsquo;t need to be exact, but\nAny size smaller would need to be centered, with space on all sides. Any size larger would appear the exact same, but would take more time to load. Any change in ratio would mean extra space (either horizontally or vertically) Default Widget Style Ideally, different widgets should use a similar style, so that if multiple widgets from different source are combined, they provide a consistent use experience. To achieve this, digital.auto recommends the following:\nUse the following font: https://www.w3schools.com/cssref/css_websafe_fonts.php Use only the colors defined on the following table, plus black/white Using plugins In order to use a plugin, it needs to be imported and configured. The configuration is necessary so that the playground understands which prototypes should be used, and how to map them to the playground`s grid-based widget layout. The following example is configuring the use of two plugins, InstrumentPanel and SmartPhone:\n[\r{\r\u0026#34;boxes\u0026#34;: [2, 3],\r\u0026#34;plugin\u0026#34;: \u0026#34;InstrumentPanel\u0026#34;,\r\u0026#34;widget\u0026#34;: \u0026#34;Speedometer\u0026#34;\r},\r{\r\u0026#34;boxes\u0026#34;: [1],\r\u0026#34;plugin\u0026#34;: \u0026#34;SmartPhone\u0026#34;,\r\u0026#34;widget\u0026#34;: \u0026#34;Image\u0026#34;\r}\r] In this example, SmartPhone will occupy the first position in the playground grid, the Instrument panel the 2nd and 3rd.\nIn order now to use these plugins in Python, the SdV prototype has to import plugins, like in the following example:\nfrom ACME_Car_ICE_v01 import Vehicle\rimport plugins\rplugins.SmartPhone.set_text(\u0026#34;Added text to SmartPhone\u0026#34;)\rvehicle = Vehicle()\rawait vehicle.Cabin.InstrumentPanel.Status.set(f\u0026#34;TEXT IN InstrumentPanel\u0026#34;) In this example, a text is added via a proprietary API to a the mockup of a Smart Phone. This is because a Smart Phone is not part of VSS. Next, a text is added to Vehicle.Cabin.InstrumentPanel.Status. In this case, InstrumentPanel.Status was added to the VSS API as a Wishlist-Item, because this could actually make sense from a VSS perspective.\nImplementing plugins A plugin is made up of a Python module with one default exported function, that takes two deconstructed object parameters: widgets and simulator. Both are described in the following:\nWidgets Widgets has one method, register that allows you to register widgets that can used by all the prototypes in the model through the Widgets Config:\nregister(widget_name: string, onActivate: (container: Container) =\u0026gt; undefined | WidgetDeactivateFunction) =\u0026gt; undefined Container Container has 3 properties:\n# Argument Description 1 injectHTML(html: string) =\u0026gt; void injectHTML is used to inject html in the box, essentially setting its innerHTML 2 injectNode(node: Node) =\u0026gt; void injectNode is used to inject an HTML Node (element or fragment) in the box. This is usually needed for complex use cases that injectHTML can\u0026rsquo;t be used for, like event listeners. 3 window The window object of the container iframe. WidgetDeactivateFunction() =\u0026gt; void\nThe deactivate function is executed whenever a widget is removed from the grid. This is useful for clearing stuff such as intervals\nSimulator simulator(api: string, method: \u0026#34;get\u0026#34; | \u0026#34;set\u0026#34; | \u0026#34;subscribe\u0026#34;, func: SimulatorModifier) =\u0026gt; void Simulator is a function that let\u0026rsquo;s you override any VSS API\u0026rsquo;s get , set and subscribe methods\nIt accepts 3 parameters:\napi : The name of the VSS API to override, for example: Vehicle.Body.Hood.IsOpen method : The method to override, this can be only one of get, set, or subscribe. func : The modifier that is run when the API method is called, of SimulatorModifier type SimulatorModifer is called with two deconstructed object parameters:\nargs : Array of arguments passed in the Python code to the method, converted to equivalent JS types. prevReturnValue : The return value this method would return if the prevReturnValue is undefined. Multiple plugins can each attach multiple modifiers to a method, all these modifiers will be called in the same order they were attached. The return value of the previous modifier will be passed to prevReturnValue.\nWidgets Config The widgets config is a JSON array of GridItem objects specified in the code tab of the prototype.\nGridItem has 3 properties:\nboxes: (number[]): The boxes this grid item should occupy. Boxes must be adjacent (horizontally or vertically) and for now, a grid item can occupy a maximum of 2 boxes.\nplugin: (string): The name of the plugin for this widget. This is the plugin name specified when creating a plugin.\nwidget: (string): The widget name. This is specified in the plugin code when registering a widget\nSummary The following provides an overview of all elements involved: A vehicle model in the playground includes one instance of a VSS catalogue (e.g. the YAML definition file with all the VSS definitions), n number of plugin implementations, and n number of SdV prototypes.\nThe VSS catalogue can be extended to use VSS Wishlist items, defined ad-hoc by different prototypes. digital.auto and COVESA are currently working on a way to submit items from the VSS Wishlist to the COVESA process for standardization of VSS.\nPlugins are defining simulators and widgets. Widgets are providing the UX for simulators (defined in the same of other plugins). In the future, each plugin will be associated with one SdV prototype for documentation purposes, as well as for defining the VSS wishlist APIs which might be required by the plugin.\nSdV prototypes are using plugins. In order to use a plugin, the configuration will have to state where exactly the required widgets should be played on the prototype-specific version of the playground grid.\n"
},
{
	"uri": "/basics/play/",
	"title": "Playing with Prototypes",
	"tags": [],
	"description": "",
	"content": "This page provides instructions on how to interact with the prototype and try it out on yourself.\nTo begin with, open the desired prototype from the model you have selected.\nThe prototype offers 5 primary sections that you can explore: Journey, Code, Dashboard, Discussion, and Feedback. In the following guide, we will walk you through each of these sections in detail.\nJourney In this section, you have the opportunity to either introduce your own prototype or read about the introductions of others. The main focus of this section is to showcase the purpose and objectives of each prototype\nCode Within the Code section, there are two parts that you will be working with: Code space and Dashboard config. The code space: In this area, you have the ability to write functions that control the prototype by getting/setting the VSS APIs. These APIs come in three types:\nBranch: Used to store actuator and sensor APIs. Actuator: Uesd to get/set/subscribed the vehicle actuator part (Electric motors, Electro-magnetic valves, etc.) Sensor: Used to get/subscribed the vehicle sensor part (Air-flow sensor, Temperature sensor, etc. ) As an illustration, let\u0026rsquo;s take a closer look at the Vehicle.Body.Lights.IsHighBeamOn Actuator API. This particular API allows you to perform the following actions:\nGet Light Value: You can retrieve the current status of the high beam lights. This provides you with information about whether the high beam lights are currently turned on or off. Set Light Value: You have the capability to set the state of the high beam lights. This means you can command the lights to either turn on or off according to your requirements. Subscribe to API: By subscribing to this API, you can receive real-time updates whenever there is a change in the status of the high beam lights. This allows you to stay informed about any modifications to the light state. This level of control over the Vehicle.Body.Lights.IsHighBeamOn Actuator API enables you to actively monitor and manipulate the high beam lights of the vehicle through the VSS API within the prototype. The dashboard config You have the ability to perform the following actions:\nBoxes: Arrange and position your widgets within designated boxes numbered from 1 to 10. This allows you to organize the visual layout of your dashboard and decide where each widget will be displayed. Plugin: Specify the name of the plugin that you wish to utilize. This defines the set of functionalities and features that will be available within your dashboard. Widget: Select and assign a registered widget from the chosen plugin to be displayed in the designated box. Widgets are visual components that provide specific information or interactive elements on your dashboard. Note: You can make edits to the Code Section only within your own prototype. Other users will only have viewing access to your prototype and won\u0026rsquo;t be able to make edits.\nDashboard This section will visualize and actualize the prototype.\nOnce you have configured the widgets and their positions using the Dashboard Configuration, these widgets will be displayed within the frames you\u0026rsquo;ve set up. Widgets that you have used will be listed in the top right corner of the dashboard.\nAfter completing the setup, simply click on the Run button to initiate testing of your prototype. To monitor and manage the outputs of your prototype, access the Control Center located on the right-hand side. Within the Control Center, you can track all the outputs generated by your prototype and also view a comprehensive list of all the invoked [VSS] APIs. It’s important to note that the execution of your prototype code follows a sequential order, from top to bottom. If you require a more detailed analysis of your code’s execution, you can utilize the Debug button. This feature allows you to step through your prototype’s code line by line, enabling a more granular understanding of its behavior. This dashboard serves as a visual representation of how your prototype is functioning and interacting with various aspects of the vehicle simulation.\nDiscussion Within the Discussion section, you will find a display of ongoing discussions and interactions among the developers involved in the creation of the prototype. This area serves as a platform for developers to communicate, exchange ideas, share insights, and address any questions or concerns related to the prototype\u0026rsquo;s development process. It facilitates collaboration and enables a transparent and organized channel for team communication.\nFeedback In the Feedback section, you have the option to both read existing feedback from users and provide your own feedback regarding the prototype. Clicking on this section allows you to access user-generated comments, suggestions, and evaluations of the prototype\u0026rsquo;s performance and features. Additionally, you can contribute by submitting your own feedback, sharing your observations, insights, and suggestions to help improve the prototype\u0026rsquo;s quality and functionality. This section serves as a valuable avenue for collecting input from users and developers to enhance the overall prototype experience.\nNote: To get the source code of python, dashboard configuration, plugin information, prototype information and related medias, click on the Export button in the top right corner of the screen\nAfter finishing the prototype, you can deploy the prototype to the KIT and experience it in real life.\n"
},
{
	"uri": "/advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": "Chapter 4 Advanced Now let\u0026rsquo;s do the complex things.\n"
},
{
	"uri": "/dreamkit/working/ecuplugplay/ecu-how-to-develop-sdv-applications/",
	"title": "Develop SDV applications to interact with your ECUs",
	"tags": [],
	"description": "",
	"content": "In this tutorial, we\u0026rsquo;ll walk you through a simple yet illustrative example: creating and deploying a simple LED blinking application to an ECU.\nBefore going into the coding and deployment, you could explore the digital.auto ecosystem to better understand the capabilities and features of dreamKIT. For a preview of what our team has done with the dreamKIT, please watch our videos. These resources provide you with insights and inspiration for your projects.\nPrerequisites To better follow the guide, you should:\nHave successfully connected your ECUs to the dreamKIT via the CAN ports. The tutorial for making connections with ECUs is here. Have a playground.digital.auto account (refer to the link here to sign up) and familiarize yourself with the playground platform. Look through the tutorial Create \u0026lsquo;Hello World\u0026rsquo; Prototype to understand more about how to create a prototype on our playground platform. Note: If you have any problem doing this tutorial, please don\u0026rsquo;t hesitate to contact us for assistance.\nWhat You Will Achieve By the end of this guide, you will understand how to develop and deploy an SDV application to your ECU. You will soon be able to create more SDV applications and showcase them with your ECU.\nLet\u0026rsquo;s get started and turn your ideas into reality with the dreamKIT! 1. Mapping VSS to DBC signals In this section, we are going to utilize the digital.auto playground platform to help us deploy the VSS to DBC signals mapping. Our SDV application written on the digital.auto playground interacts with the Vehicle Signal Specification (VSS), while our automotive ECU operates with the database CAN (DBC) signals. The mapping action allows the ECU to understand, and be able to perform your application.\n1.1. A quick look to the VSS signals As can be noticed on the list view, there are actuator and sensor signal types: Sensor type: these are signals to get values of properties in a vehicle. Actuator type: are signals used to set the value of a property. Paying attention to the VSS and DBC mapping direction is essential during application development. VSS2DBC is the actuating (set) direction, while DBC2VSS is the sensing (get) direction.\n1.2. Access the APIs mapping function For the API Mapping functionality to be available, please request us for the permission. After the edit permission of this function is granted, you can now choose to open the API Mapping tab:\nOn digital.auto playground platform please access the APIs mapping function by following the steps:\nAfter choosing the dreamKIT model, please click on Vehicle APIs: Click on the API Mapping functionality: This API Mapping tab will allow you to:\nCreate vehicle signal mapping. Deploying that mapping to dreamKIT. Export and Import mapping configurations. 1.3. Create your signal mapping Up to this stage, ECU running on CAN cannot understand what to do with the VSS signals sent via the prototype by the playground platform. Your ECU can only understand DBC signals on CAN protocol, so a mapping action is required, to translate the VSS signal to the DBC signal that your ECU can work with.\nFor example, to control the light on an ECU with the VSS API Vehicle.Body.Lights.IsLowBeamOn, the process is described as the following steps:\nInput your ECU information, including ECU Name and the corresponding Alive Message ID (this CAN message ID is unique to your ECU and should be sent periodically from your ECU at the rate of \u0026lt;= 500ms). Then map the VSS API Vehicle.Body.Lights.IsLowBeamOn with the DBC CAN signal DAS_headlightRequest on your ECU. The direction must be set to VSS2DBC to indicate that we are Actuating the light on the ECU. In case a Sensor signal is needed, DBC2VSS direction must be chosen. Then follow up by selecting your dreamKIT ID in the list. Deploy the mapping! A notification will appear if you have successfully deployed the signal mapping to the ECU: Now the target ECU can understand applications written using the configured VSS signals. The Vehicle.Body.Lights.IsLowBeamOn VSS API is also what we used in the next step of creating the LED Blinking prototype.\n2. Virtual prototyping on the digital.auto playground platform 2.1. The LED Blinking application Now we are going to create the LED Blinking prototype on digital.auto playground platform. Please access the platform, and then go through this tutorial and follow from \u0026ldquo;1. Login\u0026rdquo;, find and pick dreamKIT model in \u0026ldquo;2. Create a Vehicle Model\u0026rdquo;, and follow along to \u0026ldquo;4. Write simple headlight-blinking code\u0026rdquo;.\nThe result after following suggested tutorial is expected to be like this: The simple LED blinking code should look like this:\nfrom sdv_model import Vehicle import plugins from browser import aio vehicle = Vehicle() for i in range(10): # code to turn headlight on await self.Vehicle.Body.Lights.IsLowBeamOn.set(True) await aio.sleep(1) # code to turn headlight off await self.Vehicle.Body.Lights.IsLowBeamOn.set(False) await aio.sleep(1) 2.2. Deploy the prototype to dreamKIT The Deploy functionality will send the prototype to the dreamKIT via a network connection, so it is required to add a Wifi connection to dreamKIT to allow this over-the-air (OTA) deploying function to operate.\nFirst, make sure that dreamKIT is connected to Wifi. If the dreamKIT has not been connected to any wireless network yet, then please read the following instructions to connect one: The IVI is a touchscreen display. From the startup screen on the IVI, please swipe from the outer left side of the screen to the right to get out of the dreamKIT software window. Then access the settings button on the top right corner and tap it: The online status of our dreamKIT is now displaying \u0026ldquo;Server: connected\u0026rdquo;. Please pay close attention to the \u0026ldquo;ID:\u0026rdquo; information section, too. The last 8 digits of dreamKIT\u0026rsquo;s ID will be used to recognize your dreamKIT when deploying apps on digital.auto playground! Now let\u0026rsquo;s deploy SDV application. Go to your created prototype on Playground: Under the Code tab, click the Deploy button: If your dreamKIT is online, under the Deployment window can you find your dreamKIT model online status. Please choose your dreamKIT and finally click Deploy: Deploying\u0026hellip; A notification \u0026ldquo;There is an update from ETAS\u0026rdquo; will appear on the IVI, and your prototype will also appear in the Vehicle Apps list. The picture below shows my app that has been successfully deployed: You can now test your Vehicle App by simply sliding the button on your application on the IVI. A notification will pop up to tell you that your app has been toggled successfully: Watch how the changes you have made act on your ECU. On our dreamPACK_ECU, the LED is now blinking! 3. Create your SDV application directly on the dreamKIT Another way to develop applications for your ECU is by creating Python scripts on the dreamKIT. The Python template for SDV application development is provided by Velocitas. From this template, developers can further develop their applications based on those pre-defined templates for higher efficiency. The Python template can be accessed here.\nOur LED blinking example, when written in the form of a Python code, will look like this:\nimport asyncio import json import logging from logging.handlers import RotatingFileHandler import signal from sdv.util.log import ( get_opentelemetry_log_factory, get_opentelemetry_log_format, ) from sdv.vdb.reply import DataPointReply from sdv.vehicle_app import VehicleApp from vehicle import Vehicle, vehicle # Configure the VehicleApp logger with the necessary log config and level. logging.setLogRecordFactory(get_opentelemetry_log_factory()) logging.basicConfig(filename=\u0026#39;app.log\u0026#39;, filemode=\u0026#39;a\u0026#39;,format=\u0026#34;[%(asctime)s] %(message)s\u0026#34;) logging.getLogger().setLevel(\u0026#34;INFO\u0026#34;) logger = logging.getLogger(__name__) handler = RotatingFileHandler(\u0026#39;app.log\u0026#39;, maxBytes=1048576, backupCount=1) logger.addHandler(handler) class test_simpleLedBlinkingApp(VehicleApp): \u0026#34;\u0026#34;\u0026#34;Velocitas App for test_simpleLedBlinking.\u0026#34;\u0026#34;\u0026#34; def __init__(self, vehicle_client: Vehicle): super().__init__() self.Vehicle = vehicle_client self.home = None self.stop = None async def on_start(self): for i in range(10): await self.Vehicle.Body.Lights.IsLowBeamOn.set(True) await asyncio.sleep(1) await self.Vehicle.Body.Lights.IsLowBeamOn.set(False) await asyncio.sleep(1) async def main(): logger.info(\u0026#34;Starting test_simpleLedBlinkingApp...\u0026#34;) vehicle_app = test_simpleLedBlinkingApp(vehicle) await vehicle_app.run() LOOP = asyncio.get_event_loop() LOOP.add_signal_handler(signal.SIGTERM, LOOP.stop) LOOP.run_until_complete(main()) For a quick run please go to your Python script location in the dreamKIT file system, and then run:\n$ HOME_PATH=~ $ dapr run --app-id testapp --app-protocol grpc --resources-path $HOME_PATH/.dapr/components --config $HOME_PATH/.dapr/config.yaml --app-port 50008 python3 /your_python_directory/your_file.py$ Watch how the changes you have made act on your ECU. On our dreamPACK_ECU, the LED is now blinking! Tip: It is more advantageous to write prototypes using the manual method, as it is easier for us to integrate our Python script with third-party components into our system during the application\u0026rsquo;s development!\nNeed assistance? Contact our development team Should you have further interest to dreamKIT, please leave us some more specific query here: https://forms.office.com/e/BiJcMEhbBy\nOur team is committed to providing you with the necessary support to address your queries and challenges promptly.\nRelated Documentations To learn more about Vehicle Signal Specification (VSS): please refer to this link. To learn more about KUKSA.val, the in-vehicle software components for working with in-vehicle signals COVESA VSS: please refer to this link. To learn more on how Velocitas has helped us in faster application prototyping, please refer to the documentations provided here. "
},
{
	"uri": "/dreamkit/working/deployment/",
	"title": "Playground to dreamKIT",
	"tags": [],
	"description": "",
	"content": "\na demonstration for a simple flow from digital.auto playground to dreamKIT could be found here,\n"
},
{
	"uri": "/insider/",
	"title": "Under the Hood",
	"tags": [],
	"description": "",
	"content": "Chapter 5 Under the Hood For experts, who want to customize the playground according to different needs\n"
},
{
	"uri": "/basics/experiment/",
	"title": "Experiment",
	"tags": [],
	"description": "",
	"content": "To get started on your experimental journey, visit the playground Home and scroll down to Prototypes Gallery section\nThese are the most common prototypes help you undertand how the playground working, and what it can do.\nSelect prototype Passenger welcome and explorer what it does.\nAt the first Tab, Customer Journey, we can see the general description of this prototype, what the problem it solve, who will get impact on this issue, the complexity level, also the release status. And at the bottom, the most important thing, is Customer Journey. This table will describle the story line step by step. Who will do what, you will have the picture what going in each minor step. Now let jump to the tab Dashboard, this tab is the central point of the prototype, where \u0026ldquo;the steps\u0026rdquo; describle in tab \u0026ldquo;Customer Journey\u0026rdquo; will become true.\nTo start the show, click Run button on the right side bar and watching how the dashboard change.\nJust a simple showcase, when car owner approach his car, the proximy detect him, then it will play a sequence of actions to prepare for his drive, and his mood too: Open door, adjust seat, and turn light on.\nAlso, watching \u0026ldquo;the terminal block\u0026rdquo;, we call it widget, some message print out to welcome the owner. This is just a simple way help you debug or testing you code.\nThat\u0026rsquo;s it! A simple show to demo what should going on when you go nearby your lovely smart car. But, wait, how can those magical things happen? What is the magic behind that? Let\u0026rsquo;s open Code Tab to see more detail on code and config.\nIn Tab Code we have two main panel. The left one is where you write the logic code in python. The right panel is home of three sub tabs, APIs, Used APIs and Dashboard Config.\nLeft Panel Code tab: simple code in python, using to get/set api value Right Panel APIs: all the vehicle available for this Vehicle model. Visit \u0026lsquo;Model API\u0026rsquo; section for more detail. Used APIs: list out all APIs are currentl used in this demo, this list auto generate out when you write code. Dashboard Config: config how the dashboard look like, which widget will be used, size of widget, place to put on grid\u0026hellip;. At Code tab on the left side, try to read through the code, I believe you can understand it. Very simple, by using the Covesa VSS API, you can control the vehicle with Get()/Set().\nWould you like to try to modify to code and see the effect?\nOn the python code line #19\nprint(\u0026#34;Opening Car Door - hello Apple!\u0026#34;) Change hello Apple! to your hello your name, then give a 3 seconds for system to apply the change. Then go to Dashboard tab, click Run again and enjoy the result.\nUp to now, you can see how the playground working, but still there two more steps need your hand. Do you like the idea of this prototype, go to Tab Feedback and share your thinking there. Feedback is the place you give your thinking on the point of viewer of end user, the car owner, who using this feature, does this fearure need to have? Does it useful or not? Do you need it?\nAnd, on top right corner, which the mindset of development, there is a Discussion Panel, where you can discuss with the deverloper how the code should be improving or does the dashboard config whould look like. Discussion is design for deleloper guys to help each other.\nIf you feel that this show case is quite simple or boring, be patient, this just a simple one to help you get the idea. Go back to Prototype Gallery, there are some other beauty and complex prototypes waiting for your exploration. Enjoy!\n"
},
{
	"uri": "/releases/",
	"title": "Releases",
	"tags": [],
	"description": "",
	"content": "Chapter 6 Releases Announce Version Release with Changelogs\nv2.0.0 Date: January 6th 2025\nAvailable at: https://playground.digital.auto\nTo provide feedback or request support click here\nAt playground.digital.auto, we\u0026rsquo;ve listened to your feedback and identified some limitations in the architecture of v1. To address these, we\u0026rsquo;re excited to announce a new version with significant improvements!\nPython applications utilize the standard Velocitas library. Python applications can leverage external libraries, enabling the development of a wide range of applications. Widgets are fully plug-and-play, with no dependency on any specific model. UX improvements enhance user-friendliness. Rust Language Support: Write, compile, and execute Rust code with a single click. Supports 100 popular Rust libraries by default. New Feature: Staging replaces the previous Deploy dialog, simplifying the management of deployments from the development environment to target hardware. SDV Runtime: A Docker image serves as the execution environment for SDV applications, accommodating both Python and Rust applications. Users can either utilize numerous shared runtimes provided by us or deploy their own runtime on their local machine or in the cloud. Asset Management: Users can add and manage their own SDV runtimes or hardware kits, ensuring exclusive usage and controlled sharing. Furthermore, users can share their assets with collaborators. Several key considerations should be addressed when migrating from version 1 to version 2: view\nv1.0.0 Available at: https://digitalauto.netlify.app\nThis link will be deleted after January 17th 2025\n"
},
{
	"uri": "/dreamkit/",
	"title": "dreamKIT",
	"tags": [],
	"description": "",
	"content": "Chapter 7 dreamKit Enable the transition from virtual exploration to physical experience.\n"
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Welcome to official digital.auto documentation This page provides guideline to create SdV applications efficiently on playground.digital.auto\nFor more academic background on digitalization and AIoT, please visit https://www.digitalplaybook.org\n"
},
{
	"uri": "/advanced/ai_sdv_app/",
	"title": "AI SdV Application",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]